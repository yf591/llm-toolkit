{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmScA4Jt9+wAxXOxTDuVTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yf591/llm-toolkit/blob/main/Universal_LLM_GUI_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリーのインストールおよびインポート"
      ],
      "metadata": {
        "id": "ykPPeDPdWhok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QTtympWSQCn"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "\n",
        "!pip install transformers torch gradio\n",
        "\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # PyTorchライブラリをインポート（テンソル演算、GPU使用などに必要）\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Hugging Face Transformersライブラリから必要なクラスをインポート（言語モデル、トークナイザー）\n",
        "import gradio as gr # Gradioライブラリをインポート（Web UI作成用）"
      ],
      "metadata": {
        "id": "UaKVlIeVSjs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Current Device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "f1HMaGAYaNcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelとTokenizerをLoad"
      ],
      "metadata": {
        "id": "1AigxneiWmzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルとトークナイザーをロード\n",
        "\n",
        "#@markdown ### ◆使用するモデルの名前 (Hugging FaceのモデルID)\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  #@param {type:\"string\"}\n",
        "\n",
        "# トークナイザーをロード\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# トークナイザー：テキストをモデルが処理できる数値（トークン）に変換する役割\n",
        "# from_pretrained(): Hugging Faceから事前学習済みのトークナイザーをダウンロードしてロード\n",
        "\n",
        "# モデルをロード\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,                 # ロードするモデルの名前\n",
        "    torch_dtype=torch.float16,   # モデルのデータ型をfloat16に設定 (メモリ使用量削減)\n",
        "    device_map=\"auto\"           # 利用可能なデバイス（GPU/CPU）に自動配置\n",
        ")\n",
        "#@markdown #### - **AutoModelForCausalLM**: 因果言語モデル (テキスト生成など) のためのモデルクラス\n",
        "#@markdown #### - **from_pretrained()**: Hugging Faceから事前学習済みのモデルをダウンロードしてロード\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **※補足：torch_dtype=torch.float16 について**\n",
        "\n",
        "#@markdown float16 (半精度浮動小数点数) を使用することで、float32 (単精度浮動小数点数) の半分以下のメモリでモデルを格納できます。\n",
        "#@markdown これにより、メモリ容量が限られた環境（例：Colabの無料枠など）でも大規模モデルを使用しやすくなります。\n",
        "#@markdown ただし、精度が若干低下する可能性があるので、必要に応じてfloat32を使用することも検討してください。\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **※補足：device_map=\"auto\" について**\n",
        "\n",
        "#@markdown \"auto\" を指定することで、PyTorchが自動的に最適なデバイスを選択します。\n",
        "#@markdown GPUが利用可能な場合はGPUが選択され、そうでない場合はCPUが使用されます。\n",
        "#@markdown GPUを使用する場合は、Colabのランタイムタイプを「GPU」に設定していることを確認してください。"
      ],
      "metadata": {
        "id": "xWJiDRPLSspy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model_name` を変更することで、Hugging Faceにある様々な言語モデルを簡単に試すことができます。\n",
        "\n",
        "例えば：\n",
        "\n",
        "1. GPT系のモデル\n",
        "   - \"gpt2\"\n",
        "   - \"EleutherAI/gpt-j-6B\"\n",
        "\n",
        "2. Llama系\n",
        "   - \"meta-llama/Llama-2-7b\"\n",
        "   - \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "\n",
        "3. その他の多様なモデル\n",
        "   - \"google/flan-t5-large\"\n",
        "   - \"bigscience/bloom-560m\"\n",
        "   - \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "   \n",
        "   **※2024年にHugging Faceが発表したZephyr-7Bは、パラメータ数7BながらGPT-3.5並みの性能を達成しており、コストパフォーマンスに優れている。**\n",
        "\n",
        "**主な注意点**:\n",
        "- モデルによって推論方法や引数が若干異なる可能性\n",
        "- GPUメモリ要件が変わる\n",
        "- 性能や特性は大きく異なる\n",
        "\n",
        "**基本的なコードフレームワークは同じで、`model_name`だけ変更すれば新しいモデルを試せます。**"
      ],
      "metadata": {
        "id": "xdL8k4KYVkIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキスト生成をおこなう関数の定義"
      ],
      "metadata": {
        "id": "sXuk4tihXypz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_input_length=1024, max_output_length=200):\n",
        "    \"\"\"テキスト生成を行う関数\n",
        "\n",
        "    Args:\n",
        "        prompt (str): 入力テキスト文字列\n",
        "        max_input_length (int): 入力テキストの最大長 (デフォルト: 1024)\n",
        "        max_output_length (int): 生成するテキストの最大長 (デフォルト: 200)\n",
        "\n",
        "    Returns:\n",
        "        str: 生成されたテキスト文字列\n",
        "    \"\"\"\n",
        "    # 入力テキストをトークン化し、PyTorchテンソルに変換\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,        # 入力テキストがmax_lengthを超えた場合に切り詰める\n",
        "        max_length=max_input_length, # 入力テキストの最大長\n",
        "        return_tensors=\"pt\"     # PyTorchテンソルとして結果を返す\n",
        "    ).to(model.device)          # テンソルをモデルと同じデバイス（GPUまたはCPU）に移動\n",
        "\n",
        "    # モデルを使ってテキストを生成\n",
        "    outputs = model.generate(\n",
        "        **inputs,            # トークン化された入力をキーワード引数として展開\n",
        "        max_length=max_output_length # 生成するテキストの最大長\n",
        "    )\n",
        "\n",
        "    # 生成されたトークンIDを文字列にデコード\n",
        "    return tokenizer.decode(\n",
        "        outputs[0],         # 生成されたトークンIDのリスト（バッチサイズが1の場合）\n",
        "        skip_special_tokens=True # 特殊トークン（例：<pad>, <s>, </s>）を除外\n",
        "    )"
      ],
      "metadata": {
        "id": "awz8_xp6S9dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GUIの設定"
      ],
      "metadata": {
        "id": "MldCj2ECX3cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GUIインターフェイスの設定\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text, # 使用する関数（テキスト生成関数）\n",
        "    inputs=[         # 入力インターフェースの設定\n",
        "        gr.Textbox(\n",
        "            label=\"Prompt\",      # テキストボックスのラベル\n",
        "            lines=10,            # テキストボックスの初期表示行数\n",
        "            max_lines=20         # テキストボックスの最大行数（スクロールバーが表示される）\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=128,         # スライダーの最小値\n",
        "            maximum=2048,        # スライダーの最大値\n",
        "            value=1024,          # スライダーの初期値\n",
        "            label=\"Max Input Length\" # スライダーのラベル\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=50,          # スライダーの最小値\n",
        "            maximum=500,         # スライダーの最大値\n",
        "            value=200,           # スライダーの初期値\n",
        "            label=\"Max Output Length\" # スライダーのラベル\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"Generated Text\", # テキストボックスのラベル\n",
        "        lines=10,               # テキストボックスの初期表示行数\n",
        "        max_lines=20            # テキストボックスの最大行数（スクロールバーが表示される）\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "fTXr-mCMXw9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GUI起動\n",
        "interface.launch(share=True) # Gradioインターフェースを起動。share=Trueで外部からアクセス可能にする"
      ],
      "metadata": {
        "id": "nFWC6ToLTJ6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}