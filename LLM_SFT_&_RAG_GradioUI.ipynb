{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "TIjeFFbZi1BQ",
        "wdmgmEYIkDuW"
      ],
      "authorship_tag": "ABX9TyNHYl1RkF8IpxYry8QEb9u/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yf591/llm-toolkit/blob/main/LLM_SFT_%26_RAG_GradioUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tning と LangChainを使用したRAGの構築\n",
        "\n",
        "**LangChain関連ライブラリは主に以下の役割を担っています**\n",
        "\n",
        "*   **ドキュメントの読み込みと前処理 (RAGの情報源準備)**: PDFやテキストを読み込み、検索に適した形に分割する。\n",
        "*   **テキストのベクトル化 (埋め込み生成)**: テキスト情報を数値ベクトルに変換する。\n",
        "*   **ベクトルストアの構築と利用 (高速検索)**: ベクトル化された情報を格納し、質問に類似した情報を効率的に検索する。\n",
        "*   **LLMとの連携**: Hugging FaceのモデルをLangChainの枠組みで扱えるようにする。\n",
        "*   **プロンプト管理**: LLMへの指示を柔軟に構築する。\n",
        "*   **RAGチェーンの構築 (推論フローの自動化)**: 情報検索と回答生成の一連の処理を簡単にまとめる。\n",
        "\n",
        "これによって、RAGシステムの構築と、ファインチューニング済みモデルとの連携が効率的に行えるようになっています。"
      ],
      "metadata": {
        "id": "puLt6mqgMJdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このプロジェクトで採用した「**RAGを用いたLLMファインチューニング**」というアプローチでは例として以下のように学習を進めています。\n",
        "\n",
        "1.  **ベースモデル**\n",
        "    *   `tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1` という、既に広い知識で事前学習され、指示応答能力も持つモデルが出発点としています。\n",
        "\n",
        "2.  **ファインチューニング用データの作成プロセス (`prepare_training_data` 関数内)**\n",
        "    *   **指示データ (ichikaraデータセット)**: まず、`ichikara-instruction-003-001-1.json` から「指示（質問）」と「期待される回答の元（出力）」のペアを取得します。\n",
        "    *   **RAGによるコンテキスト検索**: 各「指示（質問）」に対して、**\"Reinforcement Learning: An Introduction\" のPDFから構築したベクトルストア（FAISSインデックス）を使って、関連性の高いテキストチャンク（これがRAGコンテキスト）を検索・取得します。**\n",
        "    *   **学習サンプルの構築**: 取得した「RAGコンテキスト」と、元の「指示（質問）」、そして `ichikara` データセットの「期待される回答」を組み合わせて、以下のような形式の1つの学習サンプル（プロンプト）を作成します。\n",
        "        ```\n",
        "        ### 指示:\n",
        "        以下のコンテキスト情報を使用して、質問に対する回答を生成してください。\n",
        "        コンテキスト情報に含まれる事実のみを使用し、含まれていない情報は推測しないでください。\n",
        "\n",
        "        ### コンテキスト:\n",
        "        { \"Reinforcement Learning: An Introduction\" から検索された関連テキストの一部 }\n",
        "\n",
        "        ### 質問:\n",
        "        { ichikaraデータセットからの指示/質問 }\n",
        "\n",
        "        ### 回答:\n",
        "        { ichikaraデータセットからの期待される出力 }\n",
        "        ```\n",
        "    *   このプロセスを `ichikara` データセットの全ての項目に対して行い、ファインチューニング用のデータセット全体を構築します。\n",
        "\n",
        "3.  **ファインチューニングの実行 (`train_model` 関数内)**\n",
        "    *   ベースモデルに対して、上記で作成した「RAGコンテキストを含むプロンプト」を入力として、「期待される回答」を教師データとして学習（ファインチューニング）を行います。\n",
        "    *   この学習によって、モデルは**「与えられたコンテキスト（この場合は強化学習の教科書からの抜粋）を理解し、それに基づいて質問に答える」という能力を獲得・向上させようと試みています。**\n",
        "\n",
        "**つまり、**\n",
        "\n",
        "*   ベースモデル (`Llama-3.1-Swallow-8B-Instruct-v0.1`) に対して、\n",
        "*   `ichikara` データセットの各指示/質問と、\n",
        "*   それに対応して \"Reinforcement Learning: An Introduction\" からRAGで検索された**一部の関連テキスト（コンテキスト）**\n",
        "*   これらがセットになったものを**学習データ（ファインチューニング用のデータ）**として使用しています。\n",
        "*   ファインチューニングモデル（ \"Reinforcement Learning: An Introduction\" ＋ ichikaraデータセットの知識を持つ）」という部分について、モデルが持つのは「知識そのもの」というよりは、「\"Reinforcement Learning: An Introduction\" からの情報を参照して、ichikara データセットのスタイルで応答する能力・パターン」 と言った方がより正確かもしれません。\n",
        "\n",
        "**重要なポイント:**\n",
        "\n",
        "*   \"Reinforcement Learning: An Introduction\" の**全部**が一度に学習データとして使われるわけではありません。各 `ichikara`データセット の質問に対して、その質問に**関連する部分だけ**がRAGによって都度抽出され、コンテキストとして学習サンプルに組み込まれます。\n",
        "*   ファインチューニングの目的は、モデルにRAGで提供される専門的なコンテキストを効果的に利用して、より質の高い回答を生成する能力を植え付けることです。"
      ],
      "metadata": {
        "id": "NP-dLyCfL_yH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## このColab Notebookの使い方\n",
        "\n",
        "1.  **セットアップ**\n",
        "    *   Google Colabのランタイムタイプを「GPU」 (A100など高性能なもの) に設定します。\n",
        "    *   セル1から順番に実行していきます。\n",
        "    *   **セル3**で、`DATA_PATH` (学習用JSON) と `DOCS_PATH` (RAG用ドキュメント/ディレクトリ) を、Colabにアップロードした実際のファイルのパスに正しく設定してください。\n",
        "2.  **ドキュメント処理 (セル7)**\n",
        "    *   `DOCS_PATH` に指定したドキュメントを処理し、ベクトルストア (`faiss_index`) を作成します。\n",
        "3.  **トレーニングデータ準備 (セル9)**\n",
        "    *   `DATA_PATH` のJSONと、セル7で作成したベクトルストアを使って、Fine-Tunig用のデータセットを作成します。\n",
        "4.  **ファインチューニング**\n",
        "    *   **セル13**を実行して、デフォルトパラメータでモデルをFine-Tunigします。モデルは `OUTPUT_DIR` に保存されます。\n",
        "    *   （オプション）より良い性能を目指す場合は、**セル14**の `DO_HYPERPARAMETER_TUNING = True` に設定して実行し、ハイパーパラメータ探索を行います。ただし、これにはかなり時間がかかるので注意してください。\n",
        "5.  **推論 (セル16)**\n",
        "    *   Fine-Tunigが完了したら、セル16を実行します。\n",
        "    *   これにより、Fine-Tunig済みモデルとRAGシステムがロードされ、Gradioのチャットインターフェースが起動します。\n",
        "    *   表示されたUIのテキストボックスに質問を入力して、チャットボットと対話ができます。\n",
        "\n",
        "**注意点**\n",
        "\n",
        "*   **メモリ**: ここで使用しているLlama3 8Bモデルでも、特にFine-Tunig中は多くのGPUメモリを消費します。Colab Pro A100であれば動作する可能性は高いですが、バッチサイズやシーケンス長などのパラメータ調整が必要になる場合があります。\n",
        "*   **時間**: ドキュメント処理、特にFine-Tunigには時間がかかります。\n",
        "*   **ファイルパス**: Colabのファイルシステム (`/content/`) 上のパスを正しく指定することが重要です。MyDriveをマウントした場合も然り。\n",
        "*   **エラー対処**: 作成時点では最後まで実行できましたが、システム等のアップデートなどで各セルでエラーが発生した場合、そのセルのエラーメッセージを確認し、必要な修正を行ってから再実行してください。前のセルの結果に依存している場合があるので、関連するセルも確認が必要です。\n",
        "*   **Gradio**: Colab上でGradio UIを実行すると、セルの出力としてUIが表示されそこでチャットができるようになります。"
      ],
      "metadata": {
        "id": "b4ynTSr_m62L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 事前準備"
      ],
      "metadata": {
        "id": "0AepioSqeuMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの確認\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "s8Xu_bZ7NbJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, output\n",
        "\n",
        "# Google Driveをマウント（必要に応じて）\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OxIwH8HjNUdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Faceへのログインは以下の方法1～方法3のいずれかの方法でおこなうこと"
      ],
      "metadata": {
        "id": "PeE4H1JcXJl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face ログイン方法1\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Hugging Faceにログイン（方法2のColabのシークレットを利用してもよい。これはお好みで。）\n",
        "login()  # トークンの入力を求められます"
      ],
      "metadata": {
        "id": "ragxS434N8vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face ログイン方法2\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata # Google Colabのuserdataモジュールをインポート\n",
        "\n",
        "# HuggingFaceアカウントにログイン\n",
        "login(userdata.get('HF_TOKEN')) # Colabのシークレットキーを使用（Hugging FaceのAPIトークンを設定しておく必要があります。）"
      ],
      "metadata": {
        "id": "qAfKRDQDZMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face ログイン方法3\n",
        "\n",
        "# 上記1または2の方法ででHugging Faceにログインできない場合は、\".env\"ファイルを作成してログインする\n",
        "# HUGGINGFACE_TOKEN=\"*******************************\"と書いて保存した.envファイルを\"/content/.env\"に置く\n",
        "\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "!huggingface-cli login --token $$HF_WRITE_TOKEN"
      ],
      "metadata": {
        "id": "IwBy2rXPObda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.環境設定とライブラリインストール\n",
        "\n",
        "- --quiet の部分は、pip (Python Package Installer) に対して、インストール時の詳細な出力（ログ）を抑制するように指示するオプション\n",
        "\n",
        "**インストールした主なLangChain関連ライブラリについて補足**\n",
        "\n",
        "*   `langchain` (コアライブラリ)\n",
        "*   `langchain-community` (コミュニティによるインテグレーション、モデル、ローダーなど)\n",
        "*   `faiss-cpu` (FAISSベクトルストアのCPU版。LangChainからベクトルストアとして利用)\n",
        "*   `pypdf` (LangChainの`PyPDFLoader`がPDF読み込みに内部で使用)"
      ],
      "metadata": {
        "id": "WvehLyFWYnwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k4QPYyC8YsE"
      },
      "outputs": [],
      "source": [
        "# 環境設定とライブラリインストール\n",
        "\n",
        "# PyTorch (Colabのデフォルト、または必要ならバージョン指定)\n",
        "# 今回はColabのデフォルトに任せる\n",
        "\n",
        "# 主要なHugging Faceライブラリ\n",
        "!pip install transformers datasets accelerate --quiet\n",
        "\n",
        "# LangChain関連\n",
        "!pip install langchain langchain-community faiss-cpu sentencepiece --quiet\n",
        "\n",
        "# QLoRA関連\n",
        "!pip install bitsandbytes peft --quiet\n",
        "\n",
        "# その他ユーティリティ\n",
        "!pip install torch --quiet # torchを明示的に再度入れることで依存関係を再確認させる場合があるが、通常は不要かも\n",
        "!pip install ray[tune] --quiet\n",
        "!pip install pandas numpy scikit-learn tqdm --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install gradio --quiet\n",
        "!pip install protobuf --quiet # 明示的に追加\n",
        "\n",
        "# 最後に fsspec と gcsfs のバージョンを強制的に調整\n",
        "# datasets 3.6.0 (仮にこのバージョンがインストールされたとする) は fsspec[http]<=2025.3.0,>=2023.1.0 を要求\n",
        "# なので、fsspec は 2025.3.0 に固定する\n",
        "print(\"Attempting to install fsspec==2025.3.0...\")\n",
        "!pip install fsspec==2025.3.0 --quiet\n",
        "# 次に、fsspec==2025.3.0 と互換性のある gcsfs (例: 2025.3.0) を指定\n",
        "# このインストールは、もし上位のバージョンが入ってしまっていたらダウングレードする効果がある\n",
        "print(\"Attempting to install gcsfs==2025.3.0...\")\n",
        "!pip install gcsfs==2025.3.0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.ライブラリのインポート\n",
        "このセルでは、インストールしたライブラリやPythonの標準モジュールをインポートします。\n",
        "\n",
        "### 主なLangChain関連ライブラリについて補足\n",
        "`from langchain_community.embeddings import HuggingFaceEmbeddings`\n",
        "- Hugging Faceのモデルを使ってテキストの埋め込みベクトルを生成するためのクラス。RAGでドキュメントや質問をベクトル化するのに使います。\n",
        "\n",
        "`from langchain_community.vectorstores import FAISS`\n",
        "- FAISSベクトルストアを扱うためのクラス。ベクトル化されたドキュメントを格納し、類似度検索を可能にします。\n",
        "\n",
        "`from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader`\n",
        "- PDFファイルやテキストファイル、ディレクトリ内のドキュメントを読み込むためのクラス。\n",
        "\n",
        "`from langchain.text_splitter import RecursiveCharacterTextSplitter`\n",
        "- 長いドキュメントを検索に適した小さなチャンクに分割するためのクラス。\n",
        "\n",
        "`from langchain.chains import RetrievalQA`\n",
        "- RAG（Retrieval Augmented Generation）の処理フローを簡単に構築するためのチェーン。リトリーバー（情報検索）とLLM（言語モデルによる回答生成）を組み合わせます。\n",
        "\n",
        "`from langchain_community.llms import HuggingFacePipeline`\n",
        "- Hugging Faceの`pipeline`（このノートブックではファインチューニング済みモデルを使ったテキスト生成パイプライン）をLangChainのLLMインターフェースに適合させるためのクラス。\n",
        "\n",
        "`from langchain.prompts import PromptTemplate`\n",
        "- LLMに入力するプロンプトのテンプレートを定義・管理するためのクラス。\n",
        "\n",
        "`from langchain_core.documents import Document`\n",
        "- LangChainがドキュメントを扱う際の基本となるデータ構造。ダミーデータ作成時などに使用。"
      ],
      "metadata": {
        "id": "mcH3aACIY7wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    logging as hf_logging, # transformers.logging\n",
        "    pipeline\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline # 正しいインポートパス\n",
        "from langchain.prompts import PromptTemplate\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import gradio as gr # Gradio UI用\n",
        "import json # JSONファイルの読み込み用\n",
        "from langchain_core.documents import Document # ダミーDocument用"
      ],
      "metadata": {
        "id": "dpjgMbsX92ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.基本設定とグローバル変数\n",
        "ここでは、モデルのパス、ディレクトリ、学習パラメータなど、プロジェクト全体で使用する主要な設定値を定義しています。ご自分の環境や目的に合わせて値を調整してください。\n",
        "\n",
        "---\n",
        "⚠️ 注意\n",
        "\n",
        "DATA_PATH と DOCS_PATH には、Colabにアップロードしたファイルの正しいパスを指定してください。\n",
        "Colabの左側にあるファイルアイコンからファイルをアップロードし、パスを右クリックしてコピーできます。また、MyDriveをマウントしてそこから読み取るように設定してもOKです。\n",
        "\n",
        "- 例: /content/ichikara-instruction-003-001-1.json\n",
        "- 例: /content/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
        "\n",
        "- (単一PDFの場合)\n",
        "  - 例: /content/my_documents_folder/ (複数ドキュメントを格納したフォルダの場合)"
      ],
      "metadata": {
        "id": "8vkKAVk9bH2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 基本設定とグローバル変数\n",
        "\n",
        "# --- 基本モデル設定 ---\n",
        "# ベースモデル\n",
        "BASE_MODEL = \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1\" #@param {type:\"string\"}\n",
        "\n",
        "# 埋め込みモデル\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-large\"   #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# --- パス設定 ---\n",
        "# トレーニングデータのパス（例: Colabにアップロードしたファイルのパス）\n",
        "DATA_PATH = \"/content/ichikara-instruction-003-001-1.json\" #@param {type:\"string\"}\n",
        "\n",
        "# RAG用ドキュメントのパス (単一ファイルまたはディレクトリ)\n",
        "DOCS_PATH = \"/content/Reinforcement Learning An Introduction Second edition.pdf\"  #@param {type:\"string\"}\n",
        "\n",
        "# ファインチューニング済みモデルの保存先\n",
        "OUTPUT_DIR = \"/content/finetuned_model\" #@param {type:\"string\"}\n",
        "\n",
        "# FAISSインデックスの保存場所\n",
        "FAISS_INDEX_PATH = \"faiss_index\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# --- LoRA設定 ---\n",
        "LORA_R = 8 #@param {type:\"number\"}\n",
        "LORA_ALPHA = 16 #@param {type:\"number\"}\n",
        "LORA_DROPOUT = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# --- RAGドキュメント処理設定 ---\n",
        "CHUNK_SIZE = 1000 #@param {type:\"number\"}\n",
        "CHUNK_OVERLAP = 200 #@param {type:\"number\"}\n",
        "\n",
        "# PDFごとに処理する最大ページ数 (メモリ対策)\n",
        "MAX_PAGES_PER_PDF = 352 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# --- トレーニングパラメータ (デフォルト) ---\n",
        "# これらの値は tune_hyperparameters で最適化されるか、直接 train_model 関数に渡されます\n",
        "DEFAULT_PER_DEVICE_BATCH_SIZE = 2 #@param {type:\"number\"}\n",
        "DEFAULT_GRADIENT_ACCUMULATION_STEPS = 8 #@param {type:\"number\"}\n",
        "DEFAULT_LEARNING_RATE = 2e-4 #@param {type:\"number\"}\n",
        "DEFAULT_LOGGING_INTERVAL = 10 #@param {type:\"number\"}\n",
        "DEFAULT_EVALUATION_INTERVAL = 100 #@param {type:\"number\"}\n",
        "DEFAULT_CHECKPOINT_INTERVAL = 100 #@param {type:\"number\"}\n",
        "\n",
        "# エポック数 (データ量に応じて調整)\n",
        "DEFAULT_NUM_TRAIN_EPOCHS = 1 #@param {type:\"number\"}\n",
        "DEFAULT_LR_SCHEDULER_TYPE = \"cosine\" #@param {type:\"string\"}\n",
        "DEFAULT_WARMUP_RATIO = 0.03 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# --- その他 ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "\n",
        "print(f\"使用デバイス: {DEVICE}\")\n",
        "print(f\"ベースモデル: {BASE_MODEL}\")\n",
        "print(f\"埋め込みモデル: {EMBEDDING_MODEL}\")\n",
        "print(f\"ファインチューニング済みモデル出力先: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "vgHd_pJy-D4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.シード設定とロギング設定\n",
        "\n",
        "ここでは再現性のために乱数シードを設定しています。またHugging Face Transformersライブラリのロギングレベルを設定しています。"
      ],
      "metadata": {
        "id": "ME4Ah3F1cHkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シード設定\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ロギング設定\n",
        "hf_logging.set_verbosity_info() # Hugging Face Transformersのログレベル"
      ],
      "metadata": {
        "id": "hyTPNnwg98-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.量子化の設定\n",
        "\n",
        "ビット量子化（QLoRA）のためのBitsAndBytes設定を返す関数get_bnb_config を定義します。"
      ],
      "metadata": {
        "id": "yRR6GxOzcplB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytesの設定 (4ビット量子化)\n",
        "def get_bnb_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )"
      ],
      "metadata": {
        "id": "rkucaX1X-LtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.ドキュメント読み込みと処理\n",
        "\n",
        "ここでは、RAGの情報源となるドキュメントを読み込んで、前処理（チャンク分割、ベクトル化）を行います。また、FAISSベクトルストアを作成・保存する関数 load_and_process_documents も定義しています。\n",
        "\n",
        "### RAG・LangChain関連の補足\n",
        "*   `PyPDFLoader`, `TextLoader`: `DOCS_PATH` からPDFやテキストファイルを読み込み、LangChainの `Document` オブジェクトのリストとして取得します。\n",
        "*   `RecursiveCharacterTextSplitter`: 読み込んだ `Document` をチャンクに分割します。\n",
        "*   `HuggingFaceEmbeddings`: 分割されたテキストチャンクを埋め込みベクトルに変換します。\n",
        "*   `FAISS`: 埋め込みベクトル化されたチャンクを格納し、高速な類似度検索が可能なベクトルストアを構築・保存します。"
      ],
      "metadata": {
        "id": "iVkPFUeIc7CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメント読み込みと処理関数 (load_and_process_documents)\n",
        "def load_and_process_documents(docs_path, faiss_index_save_path, chunk_size, chunk_overlap, max_pages_per_pdf):\n",
        "    \"\"\"\n",
        "    RAG用のドキュメントを読み込み、テキストチャンクに分割し、\n",
        "    ベクトルストア (FAISS) を作成してローカルに保存します。\n",
        "    \"\"\"\n",
        "    print(\"ドキュメント読み込みと処理を開始...\")\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    if os.path.isfile(docs_path):\n",
        "        print(f\"単一ファイル {docs_path} を処理します。\")\n",
        "        file_ext = os.path.splitext(docs_path)[1].lower()\n",
        "        if file_ext == \".pdf\":\n",
        "            print(f\"PDFファイル {docs_path} を読み込み中...\")\n",
        "            try:\n",
        "                loader = PyPDFLoader(docs_path, extract_images=False)\n",
        "                pages = loader.load_and_split()\n",
        "                documents.extend(pages[:max_pages_per_pdf])\n",
        "                print(f\"{docs_path} から最初の {min(len(pages), max_pages_per_pdf)}/{len(pages)} ページを読み込みました。\")\n",
        "            except Exception as e:\n",
        "                print(f\"PDF読み込みエラー ({docs_path}): {e}\")\n",
        "        elif file_ext == \".txt\":\n",
        "            print(f\"テキストファイル {docs_path} を読み込み中...\")\n",
        "            try:\n",
        "                loader = TextLoader(docs_path, encoding='utf-8') # エンコーディング指定\n",
        "                documents.extend(loader.load())\n",
        "            except Exception as e:\n",
        "                print(f\"テキストファイル読み込みエラー ({docs_path}): {e}\")\n",
        "        else:\n",
        "            print(f\"サポートされていないファイル形式です: {docs_path}\")\n",
        "\n",
        "    elif os.path.isdir(docs_path):\n",
        "        print(f\"ディレクトリ {docs_path} 内のファイルを処理します。\")\n",
        "        # PDFファイル読み込み\n",
        "        pdf_files = [f for f in os.listdir(docs_path) if f.lower().endswith('.pdf')]\n",
        "        for pdf_filename in tqdm(pdf_files, desc=\"PDFドキュメント読み込み\"):\n",
        "            pdf_path_full = os.path.join(docs_path, pdf_filename)\n",
        "            print(f\"PDFファイル {pdf_path_full} を読み込み中...\")\n",
        "            try:\n",
        "                loader = PyPDFLoader(pdf_path_full, extract_images=False)\n",
        "                pages = loader.load_and_split()\n",
        "                documents.extend(pages[:max_pages_per_pdf])\n",
        "                print(f\"{pdf_filename} から最初の {min(len(pages), max_pages_per_pdf)}/{len(pages)} ページを読み込みました。\")\n",
        "            except Exception as e:\n",
        "                print(f\"PDF読み込みエラー ({pdf_path_full}): {e}\")\n",
        "\n",
        "        # テキストファイル読み込み\n",
        "        txt_files = [f for f in os.listdir(docs_path) if f.lower().endswith('.txt')]\n",
        "        for txt_filename in tqdm(txt_files, desc=\"TXTドキュメント読み込み\"):\n",
        "            txt_path_full = os.path.join(docs_path, txt_filename)\n",
        "            print(f\"テキストファイル {txt_path_full} を読み込み中...\")\n",
        "            try:\n",
        "                loader = TextLoader(txt_path_full, encoding='utf-8') # エンコーディング指定\n",
        "                documents.extend(loader.load())\n",
        "            except Exception as e:\n",
        "                print(f\"テキストファイル読み込みエラー ({txt_path_full}): {e}\")\n",
        "    else:\n",
        "        print(f\"指定されたDOCS_PATH '{docs_path}' は有効なファイルまたはディレクトリではありません。\")\n",
        "\n",
        "    if not documents:\n",
        "        print(\"読み込むドキュメントが見つかりませんでした。RAGのコンテキストは空になります。\")\n",
        "        # ダミーのドキュメントやエラー処理が必要な場合がある\n",
        "        # ここでは空のベクトルストアが作成されるのを許容する\n",
        "\n",
        "    # テキスト分割\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"{len(texts)}個のテキストチャンクを作成しました。\")\n",
        "\n",
        "    if not texts:\n",
        "        print(\"テキストチャンクが作成されませんでした。ベクトルストアは空になる可能性があります。\")\n",
        "        # FAISS.from_documentsが空のリストでエラーになるのを避けるためのダミーデータ\n",
        "        texts = [Document(page_content=\"No valid content found to create embeddings.\")]\n",
        "        print(\"警告: 有効なテキストチャンクが生成されなかったため、ダミーコンテンツでベクトルストアを初期化します。\")\n",
        "\n",
        "    # 埋め込みモデルの初期化\n",
        "    print(f\"埋め込みモデル {EMBEDDING_MODEL} をロード中...\")\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL,\n",
        "        model_kwargs={\"device\": DEVICE}\n",
        "    )\n",
        "\n",
        "    # ベクトルストアの作成と保存\n",
        "    print(\"ベクトルストアを作成中...\")\n",
        "    try:\n",
        "        vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "        vectorstore.save_local(faiss_index_save_path)\n",
        "        print(f\"ベクトルストアを {faiss_index_save_path} に作成し、保存しました。\")\n",
        "    except Exception as e:\n",
        "        print(f\"ベクトルストア作成エラー: {e}\")\n",
        "        print(\"ベクトルストアの作成に失敗しました。以降のRAG処理に影響が出る可能性があります。\")\n",
        "        return None # エラー時はNoneを返す\n",
        "\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "x-1y0FBf-NHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.ドキュメント読み込みとベクトルストア作成の実行\n",
        "\n",
        "ここでは、セル6で定義された `load_and_process_documents` 関数を呼び出して、DOCS_PATH からドキュメントを読み込み、実際にLangChainの機能を使ってベクトルストアを作成します。\n",
        "\n",
        "\n",
        "---\n",
        "⚠️ DOCS_PATH が正しく設定されていることを確認してください。"
      ],
      "metadata": {
        "id": "EoW1eNDGdoM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメント読み込みとベクトルストア作成の実行\n",
        "\n",
        "# DOCS_PATHにファイル/ディレクトリが存在するか確認\n",
        "if not os.path.exists(DOCS_PATH):\n",
        "    print(f\"エラー: DOCS_PATH '{DOCS_PATH}' が見つかりません。\")\n",
        "    print(\"Colabにドキュメントファイルまたはディレクトリをアップロードし、セル3のDOCS_PATHを正しく設定してください。\")\n",
        "    # このセル以降の処理が依存するため、ここで処理を中断することも検討\n",
        "    vectorstore = None\n",
        "else:\n",
        "    vectorstore = load_and_process_documents(\n",
        "        docs_path=DOCS_PATH,\n",
        "        faiss_index_save_path=FAISS_INDEX_PATH,\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        max_pages_per_pdf=MAX_PAGES_PER_PDF\n",
        "    )\n",
        "\n",
        "if vectorstore:\n",
        "    print(\"ベクトルストアの準備が完了しました。\")\n",
        "else:\n",
        "    print(\"警告: ベクトルストアが正常に作成されませんでした。\")"
      ],
      "metadata": {
        "id": "wf3m1DnEd3Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.トレーニングデータの準備\n",
        "\n",
        "ここでは、ファインチューニング用のデータセットを準備する関数 prepare_training_data を定義します。\n",
        "\n",
        "DATA_PATH (例: ichikara-instruction-003-001-1.json) から指示と回答のペアを読み込み、各指示に対してRAG（上記7のセルで作成した vectorstore）で関連コンテキストを検索して、学習用のプロンプトを生成します。\n",
        "\n",
        "### RAG・Langchain関連の補足\n",
        "*   `vectorstore.as_retriever()`: セル7で作成したFAISSベクトルストアから、LangChainの `Retriever` インターフェースを取得します。これを使って、トレーニングデータの各質問に関連するコンテキスト情報を検索します。\n",
        "*   `retriever.get_relevant_documents(question)`: 質問に基づいて関連ドキュメント（コンテキスト）を取得します。"
      ],
      "metadata": {
        "id": "UGhFqRbkd9ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニングデータ準備関数 (prepare_training_data)\n",
        "def prepare_training_data(data_path, vectorstore, seed):\n",
        "    \"\"\"\n",
        "    ファインチューニング用のデータセットを準備します。\n",
        "    指定されたJSONデータから指示と回答を読み込み、RAGでコンテキストを付加します。\n",
        "    \"\"\"\n",
        "    print(\"トレーニングデータ準備中...\")\n",
        "\n",
        "    raw_data_list = []\n",
        "\n",
        "    if not vectorstore:\n",
        "        print(\"警告: ベクトルストアが利用できません。RAGコンテキストなしで進めますが、品質に影響する可能性があります。\")\n",
        "        # vectorstoreがNoneの場合のダミーretriever (何も返さない)\n",
        "        class DummyRetriever:\n",
        "            def get_relevant_documents(self, query): return []\n",
        "        retriever = DummyRetriever()\n",
        "    else:\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # 関連情報を3件取得\n",
        "\n",
        "    try:\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            instruction_data_json = json.load(f)\n",
        "\n",
        "        # JSONがリスト形式であることを想定 (Hugging Face datasets.load_dataset(\"json\", ...) が期待する形式)\n",
        "        if not isinstance(instruction_data_json, list):\n",
        "            # もしルートが辞書で、キー (例: \"train\") の下にリストがある場合は調整\n",
        "            # 例: if isinstance(instruction_data_json, dict) and \"train\" in instruction_data_json:\n",
        "            #         instruction_data_json = instruction_data_json[\"train\"]\n",
        "            # 今回のichikara-instructionの形式はリストなので、そのままで良いはず\n",
        "            pass\n",
        "\n",
        "        print(f\"{data_path} から {len(instruction_data_json)} 件のデータを読み込みました。\")\n",
        "\n",
        "        for item in tqdm(instruction_data_json, desc=\"RAGコンテキスト検索とデータ整形\"):\n",
        "            question = item.get(\"text\") # JSONの \"text\" を質問として使用\n",
        "            ground_truth_answer = item.get(\"output\") # JSONの \"output\" を正解回答として使用\n",
        "\n",
        "            if question is None or ground_truth_answer is None:\n",
        "                print(f\"警告: 不完全なデータエントリをスキップします: {item}\")\n",
        "                continue\n",
        "\n",
        "            # 質問に基づいて関連ドキュメントをベクトルストアから取得\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            raw_data_list.append({\n",
        "                \"question\": question,\n",
        "                \"context\": context,\n",
        "                \"answer\": ground_truth_answer\n",
        "            })\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"エラー: トレーニングデータファイル {data_path} が見つかりません。\")\n",
        "        print(\"フォールバックとしてサンプルデータを生成します... (これはテスト用です)\")\n",
        "        # サンプルデータ (フォールバック)\n",
        "        questions = [\n",
        "            \"このプロジェクトの目的は何ですか？\", \"主要な機能はどのようなものですか？\",\n",
        "            \"どのようなユースケースがありますか？\", \"類似プロジェクトはありますか？\",\n",
        "            \"技術的な課題は何ですか？\"\n",
        "        ]\n",
        "        for question in questions:\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "            answer = f\"コンテキスト情報に基づく回答です。この回答はサンプルです。{context[:100]}...\"\n",
        "            raw_data_list.append({\"question\": question, \"context\": context, \"answer\": answer})\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"エラー: {data_path} のJSON形式が正しくありません。\")\n",
        "        return None, None # エラー時はNoneを返す\n",
        "    except Exception as e:\n",
        "        print(f\"トレーニングデータ読み込み/処理中に予期せぬエラーが発生しました: {e}\")\n",
        "        return None, None # エラー時はNoneを返す\n",
        "\n",
        "    if not raw_data_list:\n",
        "        print(\"エラー: 有効なトレーニングデータが生成されませんでした。\")\n",
        "        return None, None\n",
        "\n",
        "    raw_data_hf_dataset = Dataset.from_list(raw_data_list)\n",
        "\n",
        "    # RAGプロンプトテンプレートの作成\n",
        "    def create_rag_prompt(sample):\n",
        "        return f\"\"\"### 指示:\n",
        "以下のコンテキスト情報を使用して、質問に対する回答を生成してください。\n",
        "コンテキスト情報に含まれる事実のみを使用し、含まれていない情報は推測しないでください。\n",
        "\n",
        "### コンテキスト:\n",
        "{sample['context']}\n",
        "\n",
        "### 質問:\n",
        "{sample['question']}\n",
        "\n",
        "### 回答:\n",
        "{sample['answer']}\n",
        "\"\"\"\n",
        "\n",
        "    # データセットの変換\n",
        "    try:\n",
        "        processed_data = raw_data_hf_dataset.map(\n",
        "            lambda sample: {\"text\": create_rag_prompt(sample)}, # ここを修正\n",
        "            remove_columns=raw_data_hf_dataset.column_names\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"データセットのmap処理中にエラー: {e}\")\n",
        "        print(\"raw_data_hf_dataset の内容:\", raw_data_hf_dataset[0] if len(raw_data_hf_dataset) > 0 else \"空\")\n",
        "        return None, None\n",
        "\n",
        "    # トレーニングセットとバリデーションセットに分割\n",
        "    if len(processed_data) < 10: # データが非常に少ない場合 (閾値は調整可能)\n",
        "        print(\"警告: データセットのサンプル数が非常に少ないです。\")\n",
        "        if len(processed_data) < 2:\n",
        "            print(\"サンプル数が2未満のため、トレーニング/検証セットの分割は行いません。すべてトレーニングデータとして使用します。\")\n",
        "            train_data = processed_data\n",
        "            # 検証データがないとTrainerがエラーになる場合があるので、トレーニングデータをコピーする\n",
        "            # または、evaluation_strategy=\"no\" にするなどの対応が必要\n",
        "            val_data = processed_data # 実際の学習では推奨されない\n",
        "        else:\n",
        "            # 非常に少ない場合は、test_sizeを固定値にするか、全量をtrainにする\n",
        "            train_val_split = processed_data.train_test_split(test_size=0.1, seed=seed, shuffle=True) # shuffle=True を明示\n",
        "            train_data = train_val_split[\"train\"]\n",
        "            val_data = train_val_split[\"test\"]\n",
        "    else:\n",
        "        train_val_split = processed_data.train_test_split(test_size=0.1, seed=seed, shuffle=True)\n",
        "        train_data = train_val_split[\"train\"]\n",
        "        val_data = train_val_split[\"test\"]\n",
        "\n",
        "    print(f\"トレーニングサンプル数: {len(train_data)}\")\n",
        "    print(f\"検証サンプル数: {len(val_data)}\")\n",
        "\n",
        "    return train_data, val_data"
      ],
      "metadata": {
        "id": "PPps_opd-rco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.トレーニングデータ準備の実行\n",
        "\n",
        "ここでは、実際に prepare_training_data 関数を呼び出して、トレーニングデータと検証データを作成します。\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ DATA_PATH が正しく設定されていること、およびセル7で vectorstore が正常に作成されていることを確認してください。\n"
      ],
      "metadata": {
        "id": "hiEDdMeBehiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニングデータ準備の実行\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"エラー: DATA_PATH '{DATA_PATH}' が見つかりません。\")\n",
        "    print(\"Colabにトレーニングデータファイル (JSON) をアップロードし、セル3のDATA_PATHを正しく設定してください。\")\n",
        "    train_data, val_data = None, None\n",
        "elif vectorstore is None and \"DummyRetriever\" not in str(prepare_training_data.__globals__): # セル8が正常に実行されたか\n",
        "    print(f\"エラー: ベクトルストアが初期化されていません。セル7を先に実行してください。\")\n",
        "    train_data, val_data = None, None\n",
        "else:\n",
        "    train_data, val_data = prepare_training_data(\n",
        "        data_path=DATA_PATH,\n",
        "        vectorstore=vectorstore, # セル7で作成されたもの\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "if train_data and val_data:\n",
        "    print(\"トレーニングデータと検証データの準備が完了しました。\")\n",
        "    print(\"トレーニングデータの最初のサンプル:\", train_data[0] if len(train_data) > 0 else \"データなし\")\n",
        "else:\n",
        "    print(\"エラー: トレーニングデータまたは検証データの準備に失敗しました。ログを確認してください。\")"
      ],
      "metadata": {
        "id": "itmRGqAQfo4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.モデルのロード\n",
        "\n",
        "ここでは、ベースモデルをロードし、QLoRAのために4ビット量子化とLoRA設定を適用する関数 load_model_for_training を定義しています。"
      ],
      "metadata": {
        "id": "QhUXH9DYfxl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルロード関数 (load_model_for_training)\n",
        "def load_model_for_training(base_model_name, lora_r, lora_alpha, lora_dropout):\n",
        "    \"\"\"\n",
        "    4ビット量子化とLoRA設定を適用してベースモデルをロードします。\n",
        "    \"\"\"\n",
        "    print(f\"ベースモデル {base_model_name} をトレーニング用にロード中...\")\n",
        "\n",
        "    bnb_config = get_bnb_config() # セル5で定義\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\", # 自動的にGPUに割り当て\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            base_model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Llamaではeos_tokenをpad_tokenとして使うことが多い\n",
        "        tokenizer.padding_side = \"right\" # 右パディング\n",
        "\n",
        "        # モデルをkビットトレーニング用に準備\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "        # LoRAの設定\n",
        "        lora_config = LoraConfig(\n",
        "            r=lora_r,\n",
        "            lora_alpha=lora_alpha,\n",
        "            lora_dropout=lora_dropout,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # Llama3で一般的に対象とされるモジュール\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        model.print_trainable_parameters() # トレーニング可能なパラメータ数を表示\n",
        "\n",
        "        print(f\"モデル {base_model_name} のロードとLoRA設定が完了しました。\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"モデルロード中にエラーが発生しました: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "Cmcx3Kq3-1cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.トレーニング関数の定義\n",
        "ここでは、Hugging Face TransformersのTrainerの代わりに、PyTorchのカスタムトレーニングループを使用してモデルのファインチューニングを行う関数 train_model を定義しています。\n",
        "これは、TrainingArguments の多くの設定を直接制御できるようにするためです。"
      ],
      "metadata": {
        "id": "vXlx-nDGga9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニング関数 (train_model)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm # tqdm.auto を使うことでノートブック環境などにも対応\n",
        "import os\n",
        "import math # ステップ数計算で使用\n",
        "\n",
        "# --- ヘルパー関数: オプティマイザとスケジューラの準備 ---\n",
        "def _create_optimizer_and_scheduler(\n",
        "    model_parameters,\n",
        "    learning_rate,\n",
        "    total_training_steps,\n",
        "    warmup_steps,\n",
        "    scheduler_name\n",
        "):\n",
        "    \"\"\"オプティマイザと学習率スケジューラを作成して返します。\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model_parameters, lr=learning_rate)\n",
        "\n",
        "    if scheduler_name == \"cosine\":\n",
        "        # コサインアニーリングスケジューラ\n",
        "        # T_maxはウォームアップ後の総ステップ数\n",
        "        effective_t_max = total_training_steps - warmup_steps\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=max(1, effective_t_max) # T_maxは1以上である必要あり\n",
        "        )\n",
        "    elif scheduler_name == \"linear\":\n",
        "        # 線形ウォームアップと線形減衰を行うスケジューラ\n",
        "        def lr_lambda_linear(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            return max(\n",
        "                0.0,\n",
        "                float(total_training_steps - current_step) / float(max(1, total_training_steps - warmup_steps))\n",
        "            )\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda_linear)\n",
        "    else: # デフォルトは定数スケジューラ (変更なし)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda _: 1.0)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "# --- ヘルパー関数: 1トレーニングバッチの処理 ---\n",
        "def _process_train_batch(model, data_batch, target_device, accumulation_factor):\n",
        "    \"\"\"\n",
        "    1つのトレーニングバッチを処理し、ロスを計算・返却します。\n",
        "    勾配計算もここで行います。\n",
        "    \"\"\"\n",
        "    input_ids = data_batch['input_ids'].to(target_device)\n",
        "    attention_mask = data_batch['attention_mask'].to(target_device)\n",
        "    # Causal Language Modelingでは、input_ids自体を教師ラベルとして使用\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # モデルフォワードパス\n",
        "    model_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = model_outputs.loss\n",
        "\n",
        "    # 勾配蓄積のためのロススケーリング\n",
        "    scaled_loss = loss / accumulation_factor\n",
        "    scaled_loss.backward() # スケールされたロスで勾配計算\n",
        "\n",
        "    return loss.item() # スケーリング前のバッチロスを返す\n",
        "\n",
        "# --- ヘルパー関数: モデル評価 ---\n",
        "def _run_evaluation(model, evaluation_dataloader, target_device):\n",
        "    \"\"\"検証データセットでモデルを評価し、平均ロスを返します。\"\"\"\n",
        "    model.eval() # モデルを評価モードに切り替え\n",
        "    total_loss_eval = 0\n",
        "\n",
        "    # プログレスバーの設定\n",
        "    eval_bar = tqdm(evaluation_dataloader, desc=\"Model Evaluation\", leave=False, dynamic_ncols=True)\n",
        "\n",
        "    with torch.no_grad(): # 評価中は勾配計算を無効化\n",
        "        for data_batch in eval_bar:\n",
        "            input_ids = data_batch['input_ids'].to(target_device)\n",
        "            attention_mask = data_batch['attention_mask'].to(target_device)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            model_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            batch_loss = model_outputs.loss\n",
        "            total_loss_eval += batch_loss.item()\n",
        "\n",
        "    average_eval_loss = total_loss_eval / len(evaluation_dataloader)\n",
        "    model.train() # モデルをトレーニングモードに戻す\n",
        "    return average_eval_loss\n",
        "\n",
        "# --- メイントレーニング関数 (train_model の代替) ---\n",
        "def train_model(\n",
        "    model_to_train,\n",
        "    tokenizer_for_model,\n",
        "    training_dataset,\n",
        "    validation_dataset,\n",
        "    save_path_root,\n",
        "    train_batch_size,\n",
        "    gradient_accumulation_steps,\n",
        "    max_epochs,\n",
        "    base_learning_rate,\n",
        "    lr_scheduler_name=\"cosine\",\n",
        "    warmup_ratio_of_total_steps=0.03,\n",
        "    logging_interval_steps=10,\n",
        "    evaluation_interval_steps=100,\n",
        "    checkpoint_save_interval_steps=100,\n",
        "    compute_device=\"cuda\"\n",
        "):\n",
        "    \"\"\"\n",
        "    PyTorchカスタムループを使用してモデルのトレーニングを実行します (再構成版)。\n",
        "    内部処理をヘルパー関数に分割し、変数名やコメントを変更しています。\n",
        "    \"\"\"\n",
        "    print(\"モデルトレーニングを開始 (カスタムループ - 再構成版 v2)...\")\n",
        "\n",
        "    # --- データ準備フェーズ ---\n",
        "    def _collate_and_tokenize_data(batch_samples):\n",
        "        # \"text\" フィールドに整形済みプロンプトが含まれていることを想定\n",
        "        return tokenizer_for_model(\n",
        "            batch_samples[\"text\"],\n",
        "            padding=\"max_length\", # 最大長までパディング\n",
        "            truncation=True,      # 最大長を超える場合は切り捨て\n",
        "            max_length=512       # モデルの最大コンテキスト長に合わせて調整\n",
        "        )\n",
        "\n",
        "    print(\"トレーニングデータをトークン化・フォーマット中...\")\n",
        "    processed_train_data = training_dataset.map(\n",
        "        _collate_and_tokenize_data, batched=True, remove_columns=[\"text\"]\n",
        "    )\n",
        "    processed_train_data.set_format(\"torch\")\n",
        "\n",
        "    print(\"検証データをトークン化・フォーマット中...\")\n",
        "    processed_val_data = validation_dataset.map(\n",
        "        _collate_and_tokenize_data, batched=True, remove_columns=[\"text\"]\n",
        "    )\n",
        "    processed_val_data.set_format(\"torch\")\n",
        "\n",
        "    # DataLoaderの作成\n",
        "    train_dl = DataLoader(processed_train_data, batch_size=train_batch_size, shuffle=True)\n",
        "    val_dl = DataLoader(processed_val_data, batch_size=train_batch_size) # 検証も同じバッチサイズ\n",
        "\n",
        "    # --- トレーニング設定の計算 ---\n",
        "    # 1エポックあたりのオプティマイザステップ数\n",
        "    optimizer_steps_per_epoch = math.ceil(len(train_dl) / gradient_accumulation_steps)\n",
        "    # 総オプティマイザステップ数 (トレーニング全体)\n",
        "    total_optimizer_steps = optimizer_steps_per_epoch * max_epochs\n",
        "    # ウォームアップステップ数\n",
        "    num_warmup_optimizer_steps = int(total_optimizer_steps * warmup_ratio_of_total_steps)\n",
        "\n",
        "    print(f\"計画された総オプティマイザステップ数: {total_optimizer_steps}\")\n",
        "    print(f\"うち、ウォームアップステップ数: {num_warmup_optimizer_steps}\")\n",
        "\n",
        "    # オプティマイザとスケジューラのインスタンス化\n",
        "    optimizer, lr_scheduler = _create_optimizer_and_scheduler(\n",
        "        model_to_train.parameters(),\n",
        "        base_learning_rate,\n",
        "        total_optimizer_steps,\n",
        "        num_warmup_optimizer_steps,\n",
        "        lr_scheduler_name\n",
        "    )\n",
        "\n",
        "    model_to_train.to(compute_device) # モデルを計算デバイスに移動\n",
        "\n",
        "    # --- トレーニング状態変数の初期化 ---\n",
        "    min_validation_loss_achieved = float('inf')\n",
        "    optimizer_steps_completed = 0 # 実際にoptimizer.step()が実行された回数\n",
        "\n",
        "    # --- メイントレーニングループ ---\n",
        "    for current_epoch in range(int(max_epochs)):\n",
        "        model_to_train.train() # エポック開始時にモデルをトレーニングモードに設定\n",
        "        running_epoch_loss = 0.0 # このエポックの累積ロス\n",
        "\n",
        "        # エポックごとのトレーニングプログレスバー\n",
        "        epoch_train_bar = tqdm(\n",
        "            enumerate(train_dl),\n",
        "            total=len(train_dl),\n",
        "            desc=f\"Epoch {current_epoch + 1}/{int(max_epochs)} [Training]\",\n",
        "            dynamic_ncols=True # ターミナル幅に応じてバーの長さを調整\n",
        "        )\n",
        "\n",
        "        for batch_index, current_batch in epoch_train_bar:\n",
        "            # 1バッチのトレーニング処理を実行\n",
        "            batch_loss_value = _process_train_batch(\n",
        "                model_to_train, current_batch, compute_device, gradient_accumulation_steps\n",
        "            )\n",
        "            running_epoch_loss += batch_loss_value\n",
        "\n",
        "            # 勾配蓄積ステップ数に達したら、パラメータを更新\n",
        "            if (batch_index + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()    # オプティマイザによるパラメータ更新\n",
        "                lr_scheduler.step() # 学習率スケジューラによる更新\n",
        "                optimizer.zero_grad() # 勾配をリセット\n",
        "                optimizer_steps_completed += 1\n",
        "\n",
        "                # --- 定期的なアクション (ロギング、評価、保存) ---\n",
        "                # ロギング\n",
        "                if optimizer_steps_completed % logging_interval_steps == 0:\n",
        "                    current_learning_rate = optimizer.param_groups[0]['lr']\n",
        "                    # 蓄積ステップ間の平均ロス（概算）\n",
        "                    avg_loss_since_last_log = running_epoch_loss / (batch_index + 1) if batch_index > 0 else batch_loss_value\n",
        "                    print(\n",
        "                        f\"Opt. Step: {optimizer_steps_completed}, LR: {current_learning_rate:.3e}, \"\n",
        "                        f\"Avg Train Loss (epoch part): {avg_loss_since_last_log / gradient_accumulation_steps:.4f}\"\n",
        "                    )\n",
        "\n",
        "                # 評価\n",
        "                if evaluation_interval_steps > 0 and optimizer_steps_completed % evaluation_interval_steps == 0:\n",
        "                    current_validation_loss = _run_evaluation(model_to_train, val_dl, compute_device)\n",
        "                    print(f\"Opt. Step {optimizer_steps_completed}: Validation Loss = {current_validation_loss:.4f}\")\n",
        "\n",
        "                    # 最良モデルの更新と保存\n",
        "                    if current_validation_loss < min_validation_loss_achieved:\n",
        "                        min_validation_loss_achieved = current_validation_loss\n",
        "                        best_model_save_dir = os.path.join(save_path_root, \"best_performing_model\")\n",
        "                        os.makedirs(best_model_save_dir, exist_ok=True)\n",
        "                        model_to_train.save_pretrained(best_model_save_dir)\n",
        "                        tokenizer_for_model.save_pretrained(best_model_save_dir)\n",
        "                        print(\n",
        "                            f\"New best model checkpoint saved to {best_model_save_dir} \"\n",
        "                            f\"(Validation Loss: {min_validation_loss_achieved:.4f})\"\n",
        "                        )\n",
        "\n",
        "                # 定期的なチェックポイント保存\n",
        "                if checkpoint_save_interval_steps > 0 and optimizer_steps_completed % checkpoint_save_interval_steps == 0:\n",
        "                    chkpt_save_dir = os.path.join(save_path_root, f\"checkpoint_opt_step_{optimizer_steps_completed}\")\n",
        "                    os.makedirs(chkpt_save_dir, exist_ok=True)\n",
        "                    model_to_train.save_pretrained(chkpt_save_dir)\n",
        "                    tokenizer_for_model.save_pretrained(chkpt_save_dir)\n",
        "                    print(f\"Checkpoint saved to {chkpt_save_dir} at optimizer step {optimizer_steps_completed}\")\n",
        "\n",
        "        # エポック終了時のサマリー\n",
        "        avg_epoch_loss_display = running_epoch_loss / len(train_dl)\n",
        "        print(f\"End of Epoch {current_epoch + 1}: Average Training Loss = {avg_epoch_loss_display:.4f}\")\n",
        "\n",
        "    # --- トレーニング完了後の最終処理 ---\n",
        "    final_model_save_dir = os.path.join(save_path_root, \"final_trained_model\")\n",
        "    os.makedirs(final_model_save_dir, exist_ok=True)\n",
        "    model_to_train.save_pretrained(final_model_save_dir)\n",
        "    tokenizer_for_model.save_pretrained(final_model_save_dir)\n",
        "    print(f\"Training successfully completed. Final model saved to {final_model_save_dir}\")\n",
        "\n",
        "    return model_to_train, tokenizer_for_model"
      ],
      "metadata": {
        "id": "7Y7wqJ9vtfut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.モデルのロードとトレーニングの実行 (チューニングなし)\n",
        "\n",
        "ここでは、ハイパーパラメータチューニングを行わず、セル3で定義されたデフォルトのパラメータ、または手動で調整したパラメータを使用してモデルのロードとファインチューニングを実行します。\n",
        "\n",
        "---\n",
        "⚠️ セル9で train_data, val_data が正常に準備されていることを確認してください。\n",
        "\n",
        "⚠️ セル13を実行する場合、このセルは実行しないでください\n",
        "\n",
        "⚠️ このセルを実行する場合、セル14は実行しないでください（またはその逆）。"
      ],
      "metadata": {
        "id": "5Cjrv0RdjuaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのロードとトレーニングの実行 (チューニングなし)\n",
        "\n",
        "# このセルを実行する前に、train_dataとval_dataが準備されていることを確認\n",
        "if 'train_data' not in globals() or train_data is None or \\\n",
        "   'val_data' not in globals() or val_data is None:\n",
        "    print(\"エラー: train_data または val_data が定義されていません。セル9を先に実行してください。\")\n",
        "else:\n",
        "    print(\"デフォルトパラメータでモデルのトレーニングを開始します...\")\n",
        "    # 1. モデルとトークナイザーのロード\n",
        "    model, tokenizer = load_model_for_training(\n",
        "        base_model_name=BASE_MODEL,\n",
        "        lora_r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT\n",
        "    )\n",
        "\n",
        "    if model and tokenizer:\n",
        "        # 2. トレーニングの実行\n",
        "        trained_model, trained_tokenizer = train_model(\n",
        "            model_to_train=model,\n",
        "            tokenizer_for_model=tokenizer,\n",
        "            training_dataset=train_data,\n",
        "            validation_dataset=val_data,\n",
        "            save_path_root=OUTPUT_DIR,\n",
        "            train_batch_size=DEFAULT_PER_DEVICE_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=DEFAULT_GRADIENT_ACCUMULATION_STEPS,\n",
        "            max_epochs=DEFAULT_NUM_TRAIN_EPOCHS,\n",
        "            base_learning_rate=DEFAULT_LEARNING_RATE,\n",
        "            lr_scheduler_name=DEFAULT_LR_SCHEDULER_TYPE,\n",
        "            warmup_ratio_of_total_steps=DEFAULT_WARMUP_RATIO,\n",
        "            logging_interval_steps=DEFAULT_LOGGING_INTERVAL,\n",
        "            evaluation_interval_steps=DEFAULT_EVALUATION_INTERVAL,\n",
        "            checkpoint_save_interval_steps=DEFAULT_CHECKPOINT_INTERVAL,\n",
        "            compute_device=DEVICE\n",
        "        )\n",
        "        print(f\"トレーニングが完了し、モデルは {OUTPUT_DIR} に保存されました。\")\n",
        "    else:\n",
        "        print(\"モデルのロードに失敗したため、トレーニングを実行できませんでした。\")"
      ],
      "metadata": {
        "id": "ADyBdtl2j8Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.（オプション）ハイパーパラメータチューニング\n",
        "\n",
        "ここでは、Ray Tuneを使用して最適なハイパーパラメータを探索する関数 tune_hyperparameters を定義しています。\n",
        "\n",
        "---\n",
        "⚠️  ハイパーパラメータチューニングは多くの計算リソースと時間を必要とします。小規模な試行から始めることをお勧めします。\n",
        "\n",
        "⚠️  このセルを実行する場合、セル12の実行は不要です。"
      ],
      "metadata": {
        "id": "TIjeFFbZi1BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (オプション) ハイパーパラメータチューニング関数 (tune_hyperparameters)\n",
        "def tune_hyperparameters(base_model_name, train_data, val_data, output_dir_root, device):\n",
        "    \"\"\"Ray Tuneを使用してハイパーパラメータを最適化します.\"\"\"\n",
        "    print(\"ハイパーパラメータチューニングを開始...\")\n",
        "\n",
        "    # Ray Tuneのトレーニング関数ラッパー\n",
        "    def train_with_params_for_ray(config): # configはRay Tuneから渡される\n",
        "        print(f\"Tuning with config: {config}\")\n",
        "\n",
        "        # 各試行ごとにモデルをロード\n",
        "        model, tokenizer = load_model_for_training(\n",
        "            base_model_name=base_model_name,\n",
        "            lora_r=config[\"lora_r\"], # LoRAのRもチューニング対象にする場合\n",
        "            lora_alpha=config[\"lora_r\"] * 2, # 一般的にRの2倍\n",
        "            lora_dropout=LORA_DROPOUT # 固定値またはチューニング対象\n",
        "        )\n",
        "        if model is None or tokenizer is None:\n",
        "             ray.train.report({\"val_loss\": float('inf')}) # モデルロード失敗\n",
        "             return\n",
        "\n",
        "        # トレーニングの実行 (カスタムループ版を使う)\n",
        "        # train_model関数は検証を行い、検証ロスを返すようには直接設計されていないため、\n",
        "        # ここでは最後の検証ロスをRay Tuneにレポートする簡易的な方法を取るか、\n",
        "        # train_modelを修正して定期的にval_lossをray.train.reportで報告するようにする。\n",
        "        # 簡単のため、ここではトレーニング後の最終モデルで評価する想定 (あるいはTrainerを使う)\n",
        "\n",
        "        # Trainerを使う場合の例 (元のコードに近い形)\n",
        "        temp_output_dir = os.path.join(output_dir_root, f\"tune_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{ray.train.get_context().get_trial_id()}\")\n",
        "\n",
        "        # トークン化\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "        tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "        tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "        tokenized_train.set_format(\"torch\")\n",
        "        tokenized_val.set_format(\"torch\")\n",
        "\n",
        "        # TrainingArgumentsをRay Tuneのconfigで設定\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=temp_output_dir,\n",
        "            per_device_train_batch_size=config[\"batch_size\"],\n",
        "            per_device_eval_batch_size=config[\"batch_size\"], # 評価も同じバッチサイズ\n",
        "            gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "            learning_rate=config[\"learning_rate\"],\n",
        "            num_train_epochs=config[\"num_epochs\"],\n",
        "            lr_scheduler_type=config[\"scheduler\"],\n",
        "            warmup_ratio=config[\"warmup_ratio\"],\n",
        "            logging_steps=50, # Ray Tune中は少し多めに\n",
        "            evaluation_strategy=\"epoch\", # エポックごとに評価\n",
        "            save_strategy=\"no\", # Ray Tune中はモデルを保存しない (ディスクスペース節約)\n",
        "            report_to=\"none\", # Ray TuneがレポートするのでTrainerのレポートはオフ\n",
        "            fp16=True, # A100ならTrue\n",
        "            # auto_find_batch_size=False, # Ray Tuneでバッチサイズを探索するためFalse\n",
        "        )\n",
        "\n",
        "        from transformers import Trainer # Trainerをここでインポート\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train,\n",
        "            eval_dataset=tokenized_val,\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "\n",
        "        train_result = trainer.train()\n",
        "        eval_metrics = trainer.evaluate()\n",
        "\n",
        "        # Ray Tuneに検証ロスを報告\n",
        "        ray.train.report({\"val_loss\": eval_metrics.get(\"eval_loss\", float('inf'))})\n",
        "\n",
        "\n",
        "    # Ray Tuneの設定\n",
        "    search_space = {\n",
        "        \"learning_rate\": tune.loguniform(1e-5, 5e-4),\n",
        "        \"batch_size\": tune.choice([1, 2]), # A100でも8Bモデルならバッチサイズは小さめから\n",
        "        \"gradient_accumulation_steps\": tune.choice([4, 8, 16]),\n",
        "        \"num_epochs\": tune.choice([1, 2]), # 時間がかかるので少なめに\n",
        "        \"scheduler\": tune.choice([\"cosine\", \"linear\"]),\n",
        "        \"warmup_ratio\": tune.uniform(0.01, 0.1),\n",
        "        \"lora_r\": tune.choice([LORA_R, LORA_R*2]), # LoRAランクも試す場合\n",
        "    }\n",
        "\n",
        "    # ASHAスケジューラ (早期停止アルゴリズム)\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=2,  # 最大エポック数 (num_epochsの最大値と合わせる)\n",
        "        grace_period=1, # 最低1エポックは実行\n",
        "        reduction_factor=2\n",
        "    )\n",
        "\n",
        "    # Rayの初期化 (Colabではリソース制限に注意)\n",
        "    if ray.is_initialized():\n",
        "        ray.shutdown()\n",
        "    ray.init(num_cpus=4, num_gpus=1 if device == \"cuda\" else 0, ignore_reinit_error=True) # Colab Pro A100を想定\n",
        "\n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(train_with_params_for_ray, {\"cpu\": 4, \"gpu\": 1 if device == \"cuda\" else 0}),\n",
        "        param_space=search_space,\n",
        "        tune_config=tune.TuneConfig(\n",
        "            num_samples=5,  # 試行回数 (時間と相談)\n",
        "            scheduler=scheduler,\n",
        "        ),\n",
        "        run_config=ray.train.RunConfig(\n",
        "            name=\"rag_llama3_finetune\",\n",
        "            stop={\"training_iteration\": 2}, # 最大3エポック/イテレーションで停止 (num_epochsと合わせる)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    results = tuner.fit()\n",
        "\n",
        "    best_result = results.get_best_result(metric=\"val_loss\", mode=\"min\")\n",
        "    best_config = best_result.config\n",
        "\n",
        "    print(f\"最適なハイパーパラメータ: {best_config}\")\n",
        "    print(f\"対応する検証ロス: {best_result.metrics['val_loss']}\")\n",
        "\n",
        "    ray.shutdown()\n",
        "    return best_config"
      ],
      "metadata": {
        "id": "h_nrkVXS_gjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.(オプション) モデルのロードとハイパーパラメータチューニングの実行と、最適パラメータでのトレーニング\n",
        "\n",
        "これは、セル12で定義したハイパーパラメータチューニングを実行したい場合に限って使用してください。\n",
        "チューニング後、得られた最適なパラメータでモデルをトレーニングします。\n",
        "\n",
        "---\n",
        "⚠️ このセルを実行する場合、セル12は実行しないでください（またはその逆）。多くの時間とリソースが必要です。"
      ],
      "metadata": {
        "id": "wdmgmEYIkDuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (オプション) ハイパーパラメータチューニングの実行と、最適パラメータでのトレーニング\n",
        "\n",
        "# --- チューニングを実行するかどうかのフラグ ---\n",
        "\n",
        "# Trueにするとチューニングを実行\n",
        "DO_HYPERPARAMETER_TUNING = True #@param {type: \"boolean\"}\n",
        "\n",
        "if DO_HYPERPARAMETER_TUNING:\n",
        "    if 'train_data' not in globals() or train_data is None or \\\n",
        "       'val_data' not in globals() or val_data is None:\n",
        "        print(\"エラー: train_data または val_data が定義されていません。セル9を先に実行してください。\")\n",
        "    else:\n",
        "        print(\"ハイパーパラメータチューニングを開始します...\")\n",
        "        # tune_hyperparameters 関数は、Ray Tune内部でモデルロードを行う想定\n",
        "        # そして、最適な *設定* (config) を返す\n",
        "        best_params_config = tune_hyperparameters( # best_params -> best_params_config に変更\n",
        "            base_model_name=BASE_MODEL,\n",
        "            train_data=train_data,\n",
        "            val_data=val_data,\n",
        "            output_dir_root=OUTPUT_DIR + \"_tune_runs\", # チューニング試行の出力先\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "\n",
        "        if best_params_config: # best_params_config がNoneでないことを確認\n",
        "            print(f\"最適なハイパーパラメータ設定が見つかりました: {best_params_config}\")\n",
        "            print(\"最適なパラメータ設定でモデルをトレーニングします...\")\n",
        "\n",
        "            # --- ここでモデルとトークナイザーを再度ロード ---\n",
        "            # LoRAのRもチューニング対象にした場合、best_params_configから取得\n",
        "            current_lora_r = best_params_config.get(\"lora_r\", LORA_R) # デフォルトはグローバルLORA_R\n",
        "            # LoRA AlphaはRの2倍が一般的だが、チューニング対象にするならそれもconfigから取得\n",
        "            current_lora_alpha = best_params_config.get(\"lora_alpha\", current_lora_r * 2)\n",
        "            # LoRA Dropoutもチューニング対象ならconfigから取得\n",
        "            current_lora_dropout = best_params_config.get(\"lora_dropout\", LORA_DROPOUT)\n",
        "\n",
        "            print(f\"最適な設定でモデルをロードします: LORA_R={current_lora_r}, LORA_ALPHA={current_lora_alpha}, LORA_DROPOUT={current_lora_dropout}\")\n",
        "            model_for_final_train, tokenizer_for_final_train = load_model_for_training(\n",
        "                base_model_name=BASE_MODEL,\n",
        "                lora_r=current_lora_r,\n",
        "                lora_alpha=current_lora_alpha,\n",
        "                lora_dropout=current_lora_dropout\n",
        "            )\n",
        "            # --- モデルロードここまで ---\n",
        "\n",
        "            if model_for_final_train and tokenizer_for_final_train:\n",
        "                # 最適なパラメータでトレーニングを実行\n",
        "                trained_model, trained_tokenizer = train_model(\n",
        "                    model_to_train=model_for_final_train,\n",
        "                    tokenizer_for_model=tokenizer_for_final_train,\n",
        "                    training_dataset=train_data,\n",
        "                    validation_dataset=val_data,\n",
        "                    save_path_root=OUTPUT_DIR,\n",
        "                    train_batch_size=best_params_config[\"batch_size\"], # best_params_configから取得,\n",
        "                    gradient_accumulation_steps=best_params_config[\"gradient_accumulation_steps\"],\n",
        "                    max_epochs=best_params_config[\"num_epochs\"],\n",
        "                    base_learning_rate=best_params_config[\"learning_rate\"],\n",
        "                    lr_scheduler_name=best_params_config[\"scheduler\"],\n",
        "                    warmup_ratio_of_total_steps=best_params_config[\"warmup_ratio\"],\n",
        "                    # logging_interval_stepsなどはbest_params_configに含まれていればそれを使う\n",
        "                    # なければtrain_modelのデフォルト値を使用\n",
        "                    logging_interval_steps=best_params_config.get(\"logging_interval_steps\", 10),\n",
        "                    evaluation_interval_steps=best_params_config.get(\"evaluation_interval_steps\", 100),\n",
        "                    checkpoint_save_interval_steps=best_params_config.get(\"checkpoint_save_interval_steps\", 100),\n",
        "                    compute_device=DEVICE\n",
        "                )\n",
        "                print(f\"最適化されたパラメータでのトレーニングが完了し、モデルは {OUTPUT_DIR} に保存されました。\")\n",
        "            else:\n",
        "                print(\"モデルのロードに失敗したため、トレーニングを実行できませんでした。\")\n",
        "        else:\n",
        "            print(\"ハイパーパラメータチューニングで最適なパラメータが見つかりませんでした。\")\n",
        "else:\n",
        "    print(\"ハイパーパラメータチューニングはスキップされました (DO_HYPERPARAMETER_TUNING=False)。\")\n",
        "    print(\"セル13でデフォルトパラメータによるトレーニングを実行したか確認してください。\")"
      ],
      "metadata": {
        "cellView": "code",
        "id": "dLMI2bMOl1j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.RAG推論パイプライン作成\n",
        "\n",
        "ここでは、SFT済みモデル（LoRAアダプタをロード）とRAGシステム（FAISSベクトルストア）を組み合わせて、LangChainの推論パイプラインを作成する関数 create_rag_pipeline を定義しています。\n",
        "\n",
        "### RAG・Langchain関連の補足\n",
        "*   `HuggingFaceEmbeddings`: 推論時にもFAISSインデックスをロードするために埋め込みモデルを初期化します。\n",
        "*   `FAISS.load_local()`: 保存されたFAISSインデックスを読み込みます。\n",
        "*   `vectorstore.as_retriever()`: 読み込んだFAISSベクトルストアからリトリーバーを取得します。\n",
        "*   `HuggingFacePipeline`: ファインチューニング済みのHugging FaceモデルパイプラインをLangChainのLLMとしてラップします。\n",
        "*   `PromptTemplate`: RAG推論用のプロンプトテンプレート（コンテキスト、質問、回答の指示を含む）を定義します。\n",
        "*   `RetrievalQA.from_chain_type()`: リトリーバー、LLM、プロンプトテンプレートを組み合わせて、質問応答のためのRAGチェーン (`qa_chain`) を構築します。"
      ],
      "metadata": {
        "id": "ZvSzT3XGlCKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG推論パイプライン作成関数 (create_rag_pipeline)\n",
        "def create_rag_pipeline(base_model_name, finetuned_model_path, embedding_model_name, faiss_index_path, device):\n",
        "    \"\"\"\n",
        "    ファインチューニング済みモデルとRAGを組み合わせた推論パイプラインを作成します。\n",
        "    \"\"\"\n",
        "    print(\"RAG推論パイプラインを作成中...\")\n",
        "\n",
        "    # 1. ベクトルストアの読み込み\n",
        "    print(f\"埋め込みモデル {embedding_model_name} をロード中...\")\n",
        "    try:\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=embedding_model_name,\n",
        "            model_kwargs={\"device\": device}\n",
        "        )\n",
        "        if not os.path.exists(faiss_index_path):\n",
        "            print(f\"エラー: FAISSインデックス '{faiss_index_path}' が見つかりません。セル7を再実行してください。\")\n",
        "            return None\n",
        "\n",
        "        print(f\"FAISSインデックス {faiss_index_path} をロード中...\")\n",
        "        vectorstore = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True) # 最新版FAISS対応\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # 検索するドキュメント数\n",
        "        print(\"ベクトルストアのロード完了。\")\n",
        "    except Exception as e:\n",
        "        print(f\"ベクトルストアまたは埋め込みモデルのロード中にエラー: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 2. モデルとトークナイザーのロード (ファインチューニング済み)\n",
        "    print(f\"ファインチューニング済みモデル {finetuned_model_path} をロード中...\")\n",
        "    bnb_config = get_bnb_config() # 4ビット量子化設定\n",
        "\n",
        "    try:\n",
        "        # まずベースモデルを量子化してロード\n",
        "        base_llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # 次にLoRAアダプタをロードしてマージ\n",
        "        # finetuned_model_path はLoRAアダプタが保存されているディレクトリを指す\n",
        "        if not os.path.exists(os.path.join(finetuned_model_path, \"adapter_config.json\")):\n",
        "             print(f\"警告: {finetuned_model_path} にLoRAアダプタ設定ファイル (adapter_config.json) が見つかりません。\")\n",
        "             print(f\"ベースモデル {base_model_name} をそのまま使用します。ファインチューニングが適用されない可能性があります。\")\n",
        "             model_for_inference = base_llm_model # フォールバック\n",
        "        else:\n",
        "            print(f\"LoRAアダプタを {finetuned_model_path} からロード中...\")\n",
        "            model_for_inference = PeftModel.from_pretrained(base_llm_model, finetuned_model_path)\n",
        "            # 推論時にはマージして高速化も可能だが、ここではアダプタをアタッチしたまま使う\n",
        "            # model_for_inference = model_for_inference.merge_and_unload() # マージする場合\n",
        "\n",
        "        # トークナイザーはLoRAアダプタと一緒に保存されたもの、またはベースモデルのものを使用\n",
        "        # LoRAアダプタ保存時にtokenizerも保存されていればそちらを優先\n",
        "        if os.path.exists(os.path.join(finetuned_model_path, \"tokenizer_config.json\")):\n",
        "            tokenizer_path_for_inference = finetuned_model_path\n",
        "        else:\n",
        "            tokenizer_path_for_inference = base_model_name\n",
        "            print(f\"警告: {finetuned_model_path} にトークナイザー設定が見つかりません。ベースモデル {base_model_name} のトークナイザーを使用します。\")\n",
        "\n",
        "        tokenizer_for_inference = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_path_for_inference,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        if tokenizer_for_inference.pad_token is None: # pad_tokenがない場合\n",
        "            tokenizer_for_inference.pad_token = tokenizer_for_inference.eos_token\n",
        "\n",
        "        print(\"推論用モデルとトークナイザーのロード完了。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"推論用モデルのロード中にエラー: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 3. HuggingFacePipelineの作成\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_for_inference,\n",
        "        tokenizer=tokenizer_for_inference,\n",
        "        max_new_tokens=512,  # 生成する最大トークン数 (入力長ではない)\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        do_sample=True, # サンプリングを有効にする\n",
        "        # device=0 if device == \"cuda\" else -1 # pipelineにdevice指定 (device_map=\"auto\"なら不要かも)\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    # 4. RAGプロンプトテンプレート\n",
        "    template = \"\"\"### 指示:\n",
        "以下のコンテキスト情報を使用して、質問に対する回答を生成してください。\n",
        "コンテキスト情報に含まれる事実のみを使用し、含まれていない情報は推測しないでください。コンテキストに情報がない場合は、その旨を伝えてください。\n",
        "\n",
        "### コンテキスト:\n",
        "{context}\n",
        "\n",
        "### 質問:\n",
        "{question}\n",
        "\n",
        "### 回答:\n",
        "\"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # 5. RAG QAチェーンの作成\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\", # すべての取得コンテキストをプロンプトに含める\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True, # 参照元ドキュメントも返す\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )\n",
        "\n",
        "    print(\"RAG推論パイプラインの作成が完了しました。\")\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "T8xbEiXP_plA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16.Gradio（UI表示）のための推論関数とチャットボットインターフェース\n",
        "\n",
        "ここで、Gradio UIから呼び出される推論関数とGradioのチャットボットインターフェースを定義し、UIを起動します。\n",
        "\n",
        "---\n",
        "⚠️ このセルを実行する前に、セル13またはセル14でモデルのファインチューニングが完了し、OUTPUT_DIR にモデルが保存されていることを確認してください。\n",
        "\n",
        "⚠️ またセル7で FAISS_INDEX_PATH にベクトルストアが作成されていることを確認してください。\n",
        "\n",
        "---\n",
        "### RAG・Langchain関連の補足\n",
        "*   `global_qa_chain({\"query\": message})`: セル15で作成されたLangChainの `RetrievalQA` チェーン (`global_qa_chain`) を使って、ユーザーの質問 (`message`) に対する回答生成と参照ドキュメントの取得を行います。"
      ],
      "metadata": {
        "id": "tVlfc1-hmGRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UIのための推論関数とインターフェース (gr.Interface版)\n",
        "\n",
        "# --- グローバル変数としてQAチェーンを保持 ---\n",
        "global_qa_chain = None\n",
        "\n",
        "def initialize_qa_chain_for_gradio():\n",
        "    \"\"\"Gradio UIのためにQAチェーンを初期化 (または再初期化) します。\"\"\"\n",
        "    global global_qa_chain\n",
        "\n",
        "    best_performing_model_dir = os.path.join(OUTPUT_DIR, \"best_performing_model\")\n",
        "    final_trained_model_dir = os.path.join(OUTPUT_DIR, \"final_trained_model\")\n",
        "    output_dir_direct = OUTPUT_DIR\n",
        "    finetuned_model_dir_to_load = None\n",
        "\n",
        "    if os.path.exists(os.path.join(best_performing_model_dir, \"adapter_config.json\")):\n",
        "        finetuned_model_dir_to_load = best_performing_model_dir\n",
        "        print(f\"ファインチューニング済みモデルとして '{best_performing_model_dir}' を使用します。\")\n",
        "    elif os.path.exists(os.path.join(final_trained_model_dir, \"adapter_config.json\")):\n",
        "        finetuned_model_dir_to_load = final_trained_model_dir\n",
        "        print(f\"ファインチューニング済みモデルとして '{final_trained_model_dir}' を使用します。\")\n",
        "    elif os.path.exists(os.path.join(output_dir_direct, \"adapter_config.json\")):\n",
        "        finetuned_model_dir_to_load = output_dir_direct\n",
        "        print(f\"ファインチューニング済みモデルとしてルートディレクトリ '{output_dir_direct}' を使用します。\")\n",
        "    else:\n",
        "        print(f\"エラー: ファインチューニング済みモデルが以下のいずれのパスにも見つかりません:\")\n",
        "        print(f\"  - {best_performing_model_dir}\")\n",
        "        print(f\"  - {final_trained_model_dir}\")\n",
        "        print(f\"  - {output_dir_direct} (直下)\")\n",
        "        print(\"セル13またはセル14でトレーニングを完了し、モデルが正しく保存されているか確認してください。\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(f\"エラー: FAISSインデックスが {FAISS_INDEX_PATH} に見つかりません。セル7を完了してください。\")\n",
        "        return False\n",
        "\n",
        "    global_qa_chain = create_rag_pipeline(\n",
        "        base_model_name=BASE_MODEL,\n",
        "        finetuned_model_path=finetuned_model_dir_to_load,\n",
        "        embedding_model_name=EMBEDDING_MODEL,\n",
        "        faiss_index_path=FAISS_INDEX_PATH,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    if global_qa_chain:\n",
        "        print(\"QAチェーンの初期化が完了しました。Gradio UIを起動できます。\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"QAチェーンの初期化に失敗しました。\")\n",
        "        return False\n",
        "\n",
        "# GradioのInterfaceから呼び出される関数 (履歴なし版)\n",
        "def rag_response_for_interface(user_question):\n",
        "    \"\"\"\n",
        "    ユーザーの質問を受け取り、RAGパイプラインで応答を生成します。\n",
        "    (gr.Interface用、履歴なし)\n",
        "    \"\"\"\n",
        "    if global_qa_chain is None:\n",
        "        return \"エラー: QAチェーンが初期化されていません。Gradio UIを再起動するか、初期化処理を確認してください。\"\n",
        "\n",
        "    print(f\"\\nユーザーからの質問: {user_question}\")\n",
        "    try:\n",
        "        response_dict = global_qa_chain({\"query\": user_question})\n",
        "        answer = response_dict.get(\"result\", \"回答を生成できませんでした。\")\n",
        "\n",
        "        source_docs_info = \"\\n\\n--- 参照ドキュメント ---\\n\"\n",
        "        if response_dict.get(\"source_documents\"):\n",
        "            for i, doc in enumerate(response_dict[\"source_documents\"]):\n",
        "                source_name = doc.metadata.get('source', '不明なソース')\n",
        "                # page_contentが非常に長い場合があるので、先頭部分だけ表示\n",
        "                content_preview = doc.page_content[:200].replace('\\n', ' ') + \"...\"\n",
        "                source_docs_info += f\"{i+1}. ソース: {source_name}\\n   内容抜粋: {content_preview}\\n\"\n",
        "        else:\n",
        "            source_docs_info += \"参照ドキュメントはありませんでした。\\n\"\n",
        "\n",
        "        full_answer_with_sources = answer + source_docs_info # 回答に参照情報を付加\n",
        "\n",
        "        print(f\"LLMからの回答: {answer}\")\n",
        "        if response_dict.get(\"source_documents\"):\n",
        "             print(source_docs_info)\n",
        "        return full_answer_with_sources\n",
        "    except Exception as e:\n",
        "        print(f\"推論中にエラーが発生しました: {e}\")\n",
        "        return f\"推論エラー: {e}\"\n",
        "\n",
        "# --- Gradio UIの構築 (gr.Interfaceを使用) ---\n",
        "initialization_successful = initialize_qa_chain_for_gradio()\n",
        "\n",
        "if initialization_successful:\n",
        "    print(\"Gradioインターフェース (gr.Interface版) を起動します...\")\n",
        "\n",
        "    # Gradio UIのテーマ設定 (オプション)\n",
        "    # theme = gr.themes.Soft() # または他のテーマ gr.themes.Default() など\n",
        "\n",
        "    iface = gr.Interface(\n",
        "        fn=rag_response_for_interface, # 上で定義した応答関数\n",
        "        inputs=gr.Textbox(lines=3, placeholder=\"ここに質問を入力してください...\"),\n",
        "        outputs=gr.Textbox(label=\"回答 (参照ドキュメント情報を含む)\", lines=15), # 出力行数を調整\n",
        "        title=\"RAG Llama3 Q&A (1問1答)\",\n",
        "        description=(\n",
        "            f\"ファインチューニングされた `{BASE_MODEL}` と、`{DOCS_PATH}` の情報に基づいて回答します。\\n\"\n",
        "            \"このUIは1問1答形式で、会話の履歴は保持しません。\"\n",
        "        ),\n",
        "        # theme=theme, # テーマを適用する場合\n",
        "        allow_flagging=\"never\" # 簡単のためフラグ機能をオフにする\n",
        "    )\n",
        "\n",
        "    # ColabでGradioを起動\n",
        "    iface.launch(debug=True) # debug=Trueでエラー詳細表示\n",
        "else:\n",
        "    print(\"QAチェーンの初期化に失敗したため、Gradio UIを起動できません。\")\n",
        "    print(\"上記のエラーメッセージを確認し、必要な手順 (ドキュメント処理、トレーニング等) を実行してください。\")"
      ],
      "metadata": {
        "id": "ntSsmHocmmHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.Hugging Face Hubへのアップロード(任意)\n"
      ],
      "metadata": {
        "id": "5k66-c9og94q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw-NdBdizTFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18.ローカル環境にモデルをダウンロード（任意）"
      ],
      "metadata": {
        "id": "c5feeEZ7zRwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# ダウンロード対象のディレクトリパス\n",
        "source_directory_path = \"/content/finetuned_model/final_trained_model\"\n",
        "\n",
        "# 出力されるzipファイルの名前\n",
        "output_zip_filename = \"final_trained_model_adapter.zip\"\n",
        "# zipファイルを一時的に保存するパス\n",
        "output_zip_path = f\"/content/{output_zip_filename}\"\n",
        "\n",
        "# ディレクトリが存在するか確認\n",
        "if os.path.exists(source_directory_path) and os.path.isdir(source_directory_path):\n",
        "    print(f\"ディレクトリ '{source_directory_path}' を圧縮中...\")\n",
        "    try:\n",
        "        # shutil.make_archive(base_name, format, root_dir, base_dir)\n",
        "        # base_name: 出力ファイル名 (拡張子なし)\n",
        "        # format: 'zip', 'tar', 'gztar', 'bztar', or 'xztar'\n",
        "        # root_dir: アーカイブするファイルのルートディレクトリ (このディレクトリからの相対パスでアーカイブされる)\n",
        "        # base_dir: アーカイブするディレクトリ名 (root_dirからの相対パス)\n",
        "        # ここでは、source_directory_path の親ディレクトリを root_dir にし、\n",
        "        # source_directory_path の最後のディレクトリ名を base_dir にする\n",
        "\n",
        "        archive_base_name = \"/content/final_trained_model_adapter\" # .zip は自動で付与される\n",
        "        root_dir_for_archive = os.path.dirname(source_directory_path) # /content/finetuned_model\n",
        "        base_dir_to_archive = os.path.basename(source_directory_path) # final_trained_model\n",
        "\n",
        "        shutil.make_archive(\n",
        "            base_name=archive_base_name,\n",
        "            format='zip',\n",
        "            root_dir=root_dir_for_archive,\n",
        "            base_dir=base_dir_to_archive\n",
        "        )\n",
        "\n",
        "        # 正しいzipファイルパスを再確認 (shutil.make_archiveは拡張子を自動でつける)\n",
        "        actual_output_zip_path = archive_base_name + \".zip\" # これが /content/final_trained_model_adapter.zip になる\n",
        "\n",
        "        if os.path.exists(actual_output_zip_path):\n",
        "            print(f\"圧縮ファイル '{actual_output_zip_path}' が作成されました。\")\n",
        "            print(\"ダウンロードを開始します...\")\n",
        "            files.download(actual_output_zip_path)\n",
        "            print(f\"'{actual_output_zip_path}' のダウンロードがトリガーされました。ブラウザのダウンロードを確認してください。\")\n",
        "        else:\n",
        "            print(f\"エラー: 圧縮ファイル '{actual_output_zip_path}' が見つかりません。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"圧縮またはダウンロード中にエラーが発生しました: {e}\")\n",
        "else:\n",
        "    print(f\"エラー: 指定されたディレクトリ '{source_directory_path}' が見つからないか、ディレクトリではありません。\")"
      ],
      "metadata": {
        "id": "mIdgKAUzzQ9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}