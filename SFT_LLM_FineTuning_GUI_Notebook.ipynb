{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO2x6oeOydJKX10q1E5ItM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yf591/llm-toolkit/blob/main/SFT_LLM_FineTuning_GUI_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-3.1-Swallow-8B-Instruct-v0.1の SFTTrainer による QLoRA ファインチューニング\n",
        "\n",
        "###Overview：\n",
        "このNotebookは、例としてLlama-3.1-Swallow-8B-Instruct-v0.1モデルをQLoRA（Quantized Low-Rank Adaptation）を用いてファインチューニングする手順を示します。\n",
        "ファインチューニングには、Hugging Faceの`trl`ライブラリの`SFTTrainer`を使用します。\n",
        "\n",
        "### 目次\n",
        "1. **環境セットアップ**\n",
        "    -  GPUの確認\n",
        "    -  Googleドライブのマウント\n",
        "    -  作業ディレクトリの変更\n",
        "    -  基本パラメータの設定\n",
        "2. **ライブラリのインストール**\n",
        "    -  必要なライブラリのインストール\n",
        "    -  ライブラリのインポート\n",
        "    -  Hugging Faceへのログイン\n",
        "3. **モデルの準備**\n",
        "    -  量子化設定\n",
        "    -  モデルとトークナイザーのロード\n",
        "4. **モデルの動作確認**\n",
        "    -  初期状態での推論テスト\n",
        "5. **データセットの準備**\n",
        "    -  データセットのロード\n",
        "    -  データセットの確認\n",
        "    -  データセットのフォーマット\n",
        "6. **QLoRA設定**\n",
        "    -  線形層の名前の取得\n",
        "    -  LoRAの設定\n",
        "7. **学習設定**\n",
        "    -  学習引数の設定\n",
        "8. **ファインチューニング**\n",
        "    -  SFTTrainerによるファインチューニング\n",
        "9. **学習結果の確認**\n",
        "    -  学習したパラメータの比率の確認\n",
        "    -  GPUメモリのリセット\n",
        "10. **ファインチューニングモデルのロード**\n",
        "    -  量子化設定\n",
        "    -  モデルとトークナイザーのロード\n",
        "    -  ファインチューニングモデルのロード\n",
        "11. **推論テスト**\n",
        "    -  文章生成関数の定義\n",
        "    -  ファインチューニング後のモデルによる推論テスト\n",
        "12. **Colabランタイムの強制終了（オプション）**"
      ],
      "metadata": {
        "id": "PF99gl0dgI8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.**環境セットアップ**\n",
        "-  GPUの確認\n",
        "-  Googleドライブのマウント\n",
        "-  作業ディレクトリの変更\n",
        "-  基本パラメータの設定"
      ],
      "metadata": {
        "id": "FbmceJbogXL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUの状態を確認\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8V0-mpL4Yqfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Googleドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\") # Googleドライブをマウントし、ノートブックからファイルにアクセスできるようにします。"
      ],
      "metadata": {
        "id": "urHWXgy5YuFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 自分のGoogleドライブの作業用フォルダのパスに書き換える\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/llm_toolkit_google_colab/01_Instruction_tuning_QLoRA\n",
        "%ls # 現在のディレクトリにあるファイルを表示"
      ],
      "metadata": {
        "id": "gX2CkcWlYwaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 基本パラメータ\n",
        "model_id = \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1\" # 使用するモデルのID\n",
        "peft_name = \"Llama3.1-SW-8B-it-v0.1_A100_1rep_qlora\" # ファインチューニング後のモデルを保存する際の名前\n",
        "output_dir = \"output_neftune\" # 学習済みモデルの出力ディレクトリ"
      ],
      "metadata": {
        "id": "90SG1DzbY6qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.**ライブラリのインストール**\n",
        "-  必要なライブラリのインストール\n",
        "-  ライブラリのインポート\n",
        "-  Hugging Faceへのログイン"
      ],
      "metadata": {
        "id": "db579hKAghaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# ライブラリのインストール\n",
        "!pip install peft # PEFT（Parameter-Efficient Fine-Tuning）ライブラリ\n",
        "!pip install transformers==4.43.3\n",
        "!pip install datasets==2.14.5\n",
        "!pip install accelerate bitsandbytes evaluate\n",
        "!pip install trl==0.12.0 # TRL（Transformer Reinforcement Learning）ライブラリ,（バージョン0.12.0を指定）\n",
        "!pip install flash-attn # T4では使えないのでコメントアウト"
      ],
      "metadata": {
        "id": "KUNArTLJY_4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリーのインストール\n",
        "\n",
        "import torch # PyTorchライブラリ\n",
        "from torch import cuda, bfloat16 # PyTorchのCUDA, bfloat16関連の機能\n",
        "from transformers import ( # Transformersライブラリから必要なクラスをインポート\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "\n",
        "\n",
        "from datasets import load_dataset # Datasetsライブラリからload_dataset関数をインポート\n",
        "from peft import LoraConfig, PeftModel # PEFTライブラリから必要なクラスをインポート\n",
        "from trl import SFTTrainer # TRLライブラリからSFTTrainerをインポート"
      ],
      "metadata": {
        "id": "-WAYGtnFZKUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login # Hugging Face Hubのlogin関数をインポート\n",
        "from google.colab import userdata # Google Colabのuserdataモジュールをインポート\n",
        "\n",
        "# HuggingFaceアカウントにログイン\n",
        "login(userdata.get('HF_TOKEN')) # Colabのシークレットキーを使用（Hugging Faceのトークンを設定しておく必要があります。）"
      ],
      "metadata": {
        "id": "qAfKRDQDZMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.**モデルの準備**\n",
        "-  量子化設定\n",
        "-  モデルとトークナイザーのロード"
      ],
      "metadata": {
        "id": "FVdMmfUhgq4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 量子化設定 (量子化に関する設定)\n",
        "bnb_config = BitsAndBytesConfig( # 量子化設定用のオブジェクトを作成開始\n",
        "    load_in_4bit=True,           # モデルを4ビットで読み込む設定 (メモリ削減)\n",
        "    bnb_4bit_use_double_quant=True, # 4ビット量子化で二重量子化を使用 (精度向上)\n",
        "    bnb_4bit_quant_type=\"nf4\",     # 4ビット量子化のタイプをNF4形式に指定\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # 量子化中の計算精度をbfloat16に指定\n",
        ")\n",
        "\n",
        "# モデルの設定 (モデル読み込みの設定)\n",
        "model = AutoModelForCausalLM.from_pretrained( # 事前学習済みモデルをロード開始\n",
        "    model_id,                      # 使用するモデルのID (Hugging Face Hub上の名前など)\n",
        "    trust_remote_code=True,        # リモート(Hub上)のカスタムコード実行を許可\n",
        "    quantization_config=bnb_config,# 上記で定義した量子化設定を適用\n",
        "    device_map='auto',             # モデルをGPU/CPUに自動で割り当て\n",
        "    torch_dtype=torch.bfloat16,    # モデルの計算時のデータ型をbfloat16に指定\n",
        "    attn_implementation=\"flash_attention_2\" # 高速化技術FlashAttention2\n",
        ")\n",
        "\n",
        "# tokenizerの設定 (トークナイザー読み込みの設定)\n",
        "tokenizer = AutoTokenizer.from_pretrained( # 事前学習済みトークナイザーをロード開始\n",
        "    model_id,                      # 使用するトークナイザーのID (モデルと通常同じ)\n",
        "    padding_side=\"right\",          # パディング(穴埋め)をシーケンスの右側に行う設定\n",
        "    add_eos_token=True             # 文末に終了を示すEOSトークンを自動で追加する設定\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token_id is None: # もしパディング用トークンIDが未設定なら\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id # 終了トークンIDをパディング用IDとして使う\n"
      ],
      "metadata": {
        "id": "NnVMnsKAZPsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.**モデルの動作確認**\n",
        "- 初期状態での推論テスト"
      ],
      "metadata": {
        "id": "xRyHzsbkg5xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テスト用のメッセージを作成 (ここからモデルへの入力メッセージを作成)\n",
        "messages = [                     # チャット形式のメッセージリストを作成開始\n",
        "    {\"role\": \"system\", \"content\": \"あなたは常に日本語で応答する優秀なアシスタントです。\"}, # システムメッセージ(AIの役割指示)\n",
        "    {\"role\": \"user\", \"content\": \"広島県の美味しい食べ物や有名な建造物は何ですか？\"},         # ユーザーからの質問メッセージ\n",
        "] # メッセージリスト作成完了\n",
        "\n",
        "# 入力メッセージをトークン化し、モデルのデバイスに転送 (ここから入力データをモデル用に変換)\n",
        "input_ids = tokenizer.apply_chat_template( # チャットテンプレートを適用してトークンIDに変換開始\n",
        "    messages,                    # 変換するメッセージリスト\n",
        "    add_generation_prompt=True,  # AIの応答を促すプロンプトを追加する設定\n",
        "    return_tensors=\"pt\"          # 結果をPyTorchテンソル形式で返す設定\n",
        ").to(model.device)               # 変換結果をモデルと同じデバイス(GPU等)に移動\n",
        "\n",
        "# 文章生成を終了するトークンIDを設定 (ここで生成停止の条件を設定)\n",
        "terminators = [                  # 生成終了のトリガーとなるトークンIDのリストを作成開始\n",
        "    tokenizer.eos_token_id,      # 標準の終了(EOS)トークンID\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") # 特定の終了用トークン文字列をIDに変換して追加\n",
        "] # 生成終了トークンIDリスト作成完了\n",
        "\n",
        "# モデルを使用して文章を生成 (ここから実際に文章を生成)\n",
        "outputs = model.generate(        # モデルの`generate`メソッドで文章生成を開始\n",
        "    input_ids,                   # 生成の元となる入力トークンID\n",
        "    max_new_tokens=256,          # 生成する新しいトークン数の上限を256に設定\n",
        "    eos_token_id=terminators,    # 生成停止のトリガーとなるトークンID(リスト)を指定\n",
        "    do_sample=True,              # 次のトークンを確率的にサンプリングする方式を使う\n",
        "    temperature=0.8,             # サンプリングのランダム性を調整 (低いほど決定的)\n",
        "    top_p=0.8,                   # Top-pサンプリングを使用 (累積確率0.8までの候補から選ぶ)\n",
        "    pad_token_id=tokenizer.eos_token_id, # 生成中に使うパディングトークンIDを指定 (EOSと同じ)\n",
        "    attention_mask=torch.ones(input_ids.shape, dtype=torch.long).cuda(), # 入力部分全体に注意を向けるマスクを作成しGPUへ\n",
        ") # 文章生成完了、結果を`outputs`に格納\n",
        "\n",
        "# 生成されたトークンからレスポンスを抽出 (ここから生成結果の後処理)\n",
        "response = outputs[0][input_ids.shape[-1]:] # 生成結果(outputsの最初の要素)から入力部分を除いた応答部分を抽出\n",
        "\n",
        "# 生成されたレスポンスを整形して表示 (ここから結果を読みやすく表示)\n",
        "import textwrap                  # テキストの折り返し用ライブラリをインポート\n",
        "s = tokenizer.decode(response, skip_special_tokens=True) # 応答部分のトークンIDを文字列にデコード (特殊トークンは削除)\n",
        "s_wrap_list = textwrap.wrap(s, 50) # デコードした文字列を50文字ごとに折り返してリスト化\n",
        "print('\\n'.join(s_wrap_list))    # 折り返した文字列リストを改行で繋げて表示\n",
        "\n"
      ],
      "metadata": {
        "id": "wE0zU4RtZTgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.**データセットの準備**\n",
        "-  データセットのロード\n",
        "-  データセットの確認\n",
        "-  データセットのフォーマット"
      ],
      "metadata": {
        "id": "mZujNxWIhOdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ローカル（MyDrive上）にあるデータセットをロード\n",
        "dataset = load_dataset(\"./dataset\", split=\"train\") # データセットは、このNotebookが実行されるディレクトリにdatasetという名前のフォルダがある想定"
      ],
      "metadata": {
        "id": "e8CfdhRaZW6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの中身確認\n",
        "dataset[200]"
      ],
      "metadata": {
        "id": "IPIQLyWRZbrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの各要素を、チャット形式のメッセージに変換する関数\n",
        "def formatting_func(example):\n",
        "        messages = [\n",
        "            {'role': \"system\",'content': \"あなたは日本語で回答するアシスタントです\"},\n",
        "            {'role': \"user\",'content': example[\"instruction\"]},\n",
        "            {'role': \"assistant\",'content': example[\"output\"]}\n",
        "        ]\n",
        "        return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "\n",
        "# データセットの各要素を更新する関数\n",
        "# フォーマットされたテキストを\"text\"キーに追加し、不要なキーを削除\n",
        "def update_dataset(example):\n",
        "    example[\"text\"] = formatting_func(example)\n",
        "    for field in [\"index\", \"category\", \"instruction\", \"input\", \"output\"]:\n",
        "        example.pop(field, None)\n",
        "    return example\n",
        "\n",
        "\n",
        "# データセットを更新\n",
        "dataset = dataset.map(update_dataset)\n",
        "\n",
        "# 更新されたデータセットの11番目の要素の\"text\"を表示\n",
        "print(dataset[10][\"text\"])"
      ],
      "metadata": {
        "id": "EzmIAMa4ZfA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.**QLoRA設定**\n",
        "-  線形層の名前の取得\n",
        "-  LoRAの設定"
      ],
      "metadata": {
        "id": "9W5pP2elh0O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの情報を表示\n",
        "model"
      ],
      "metadata": {
        "id": "jPW4PzNhZiKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルから（4ビット量子化された）線形層の名前を取得する関数\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# モデルのすべての線形層の名前を取得する関数\n",
        "def find_all_linear_names(model):\n",
        "    target_class = bnb.nn.Linear4bit\n",
        "    linear_layer_names = set()\n",
        "    for name_list, module in model.named_modules():\n",
        "        if isinstance(module, target_class):\n",
        "            names = name_list.split('.')\n",
        "            layer_name = names[-1] if len(names) > 1 else names[0]\n",
        "            linear_layer_names.add(layer_name)\n",
        "    if 'lm_head' in linear_layer_names:\n",
        "        linear_layer_names.remove('lm_head')\n",
        "    return list(linear_layer_names)\n",
        "\n",
        "target_modules = find_all_linear_names(model) # 線形層の名前を取得\n",
        "print(target_modules) # 線形層の名前を表示"
      ],
      "metadata": {
        "id": "eQ9Z84i5Zjzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8, # LoRAのランク\n",
        "    lora_alpha=16, # LoRAのアルファ値\n",
        "    lora_dropout=0.05, # LoRAのドロップアウト率\n",
        "    target_modules = target_modules, # LoRAを適用するモジュール\n",
        "    bias=\"none\", # バイアス項を使用しない\n",
        "    task_type=\"CAUSAL_LM\", # タスクの種類\n",
        "    modules_to_save=[\"embed_tokens\"], # 学習後に保存するモジュール\n",
        ")\n"
      ],
      "metadata": {
        "id": "kcDyOd7_ZlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.**学習設定**\n",
        "-  学習引数の設定"
      ],
      "metadata": {
        "id": "cwj0Skljh7Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習中の評価、保存、ロギングを行う間隔を設定\n",
        "eval_steps = 20\n",
        "save_steps = 20\n",
        "logging_steps = 20\n",
        "\n",
        "# 学習引数の設定\n",
        "training_arguments = TrainingArguments(\n",
        "    bf16=True, # bfloat16を使用\n",
        "    per_device_train_batch_size=4, # デバイスごとのバッチサイズ\n",
        "    gradient_accumulation_steps=16, # 勾配累積ステップ数\n",
        "    num_train_epochs=1, # 学習エポック数\n",
        "    optim=\"adamw_torch_fused\", # 最適化アルゴリズム\n",
        "    learning_rate=2e-4, # 学習率\n",
        "    lr_scheduler_type=\"cosine\", # 学習率スケジューラ\n",
        "    weight_decay=0.01, # 重み減衰\n",
        "    warmup_steps=100, # ウォームアップステップ数\n",
        "    group_by_length=True, # 長さでグループ化\n",
        "    report_to=\"none\", # wandbへのレポートを無効化\n",
        "    logging_steps=logging_steps, # ログの記録間隔\n",
        "    eval_steps=eval_steps, # 評価間隔\n",
        "    save_steps=save_steps, # モデルの保存間隔\n",
        "    output_dir=output_dir, # 学習済みモデルの出力ディレクトリ\n",
        "    save_total_limit=3, # 保存するモデルの最大数\n",
        "    push_to_hub=False, # Hugging Face Hubへのアップロードを無効化\n",
        "    # report_to=\"wandb\"\n",
        "    auto_find_batch_size=True # GPUメモリのオーバーフロー防止（バッチサイズを自動で調整）\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "up2i16SOZndS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.**ファインチューニング**\n",
        "-  SFTTrainerによるファインチューニング"
      ],
      "metadata": {
        "id": "3Ld-MUCRiJ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SFTTrainerの初期化\n",
        "trainer = SFTTrainer(\n",
        "    model=model, # モデル\n",
        "    tokenizer=tokenizer, # トークナイザー\n",
        "    train_dataset=dataset, # 学習データセット\n",
        "    dataset_text_field=\"text\", # データセットのテキストフィールド\n",
        "    peft_config=peft_config, # PEFTの設定\n",
        "    args=training_arguments, # 学習引数\n",
        "    max_seq_length=1024, # 最大シーケンス長\n",
        "    packing=True, # パッキングを使用\n",
        "    neftune_noise_alpha=5, # NEFTune設定, NEFTuneノイズアルファ値\n",
        ")\n",
        "\n",
        "# wandb.init(project=\"llama3_sftqlora\")"
      ],
      "metadata": {
        "id": "LpUkgfIxZpg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# 学習中に警告を抑制するためにキャッシュを使用しない設定にし、学習後にキャッシュを有効にする。\n",
        "model.config.use_cache = False\n",
        "trainer.train()\n",
        "model.config.use_cache = True\n",
        "\n",
        "# 学習したQLoRAモデルを保存\n",
        "trainer.model.save_pretrained(peft_name)"
      ],
      "metadata": {
        "id": "SsEN3uHfZr4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.**学習結果の確認**\n",
        "-  学習したパラメータの比率の確認\n",
        "-  GPUメモリのリセット"
      ],
      "metadata": {
        "id": "y001wp9mjt8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習したパラメータの比率確認\n",
        "trainer.model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "61TDT1PeZuP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache() # GPUメモリをリセット"
      ],
      "metadata": {
        "id": "74PMWa9rZwXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.**ファインチューニングモデルのロード**\n",
        "-  量子化設定\n",
        "-  モデルとトークナイザーのロード\n",
        "-  ファインチューニングモデルのロード"
      ],
      "metadata": {
        "id": "5bxM5VjpjvY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 量子化設定\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# モデルの設定・ロード\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    # token=token, # HuggingFaceにログインしておけば不要\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "\n",
        "# tokenizerの設定・ロード\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    padding_side=\"right\",\n",
        "    add_eos_token=True\n",
        ")\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "5gC10kcgZxmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューニングモデルの作成\n",
        "from peft import PeftModel\n",
        "ft_model = PeftModel.from_pretrained(model, peft_name)"
      ],
      "metadata": {
        "id": "qdHRTz6tZ0uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.**推論テスト**(1～3のいずれかで実施)\n",
        "-  文章生成関数の定義\n",
        "-  ファインチューニング後のモデルによる推論テスト"
      ],
      "metadata": {
        "id": "9PpuMRiXkB5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 コードベースでテスト"
      ],
      "metadata": {
        "id": "svLs3XjxoKL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 文章生成関数を定義\n",
        "def generate(prompt):\n",
        "  # 入力プロンプトをチャット形式のメッセージに変換\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"あなたは日本語で回答するアシスタントです。\"},\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # 入力メッセージをトークン化し、モデルのデバイスに転送\n",
        "  input_ids = tokenizer.apply_chat_template( # This line had an extra indent\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(ft_model.device)\n",
        "\n",
        "  # 文章生成を終了するトークンIDを設定\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  # モデルを使用して文章を生成\n",
        "  outputs = ft_model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "      pad_token_id=tokenizer.eos_token_id, # 追加\n",
        "      attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(ft_model.device),\n",
        "  )\n",
        "\n",
        "  # 生成されたトークンからレスポンスを抽出\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "  # print(tokenizer.decode(response, skip_special_tokens=True))\n",
        "\n",
        "  # 生成されたレスポンスを整形して表示\n",
        "  import textwrap\n",
        "  s = tokenizer.decode(response, skip_special_tokens=True)\n",
        "  s_wrap_list = textwrap.wrap(s, 50) # 50字で改行したリストに変換\n",
        "  print('\\n'.join(s_wrap_list))"
      ],
      "metadata": {
        "id": "fHcUfIETZ3H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングされたモデルを使用して文章を生成\n",
        "generate(\"こんにちは。最近の調子はどうですか？\")"
      ],
      "metadata": {
        "id": "wpQUdNDWZ4vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate(\"CPUとGPUの違いは何ですか？ 詳しく教えてください。\")"
      ],
      "metadata": {
        "id": "l7WWl5CRZ52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate(\"広島では何が有名ですか？\")"
      ],
      "metadata": {
        "id": "LKsWmhcEZ8Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate(\"広島にあるプロスポーツチームを教えて？\")"
      ],
      "metadata": {
        "id": "JbnNwLFEZ9MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11.2 Gradio を利用"
      ],
      "metadata": {
        "id": "0rbhmd4joUZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "ylQ9_GUrook7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# --- 文章生成関数 (GUI対応版) ---\n",
        "def generate(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"あなたは日本語で回答するアシスタントです。\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(ft_model.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = ft_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(ft_model.device),\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    s = tokenizer.decode(response, skip_special_tokens=True)\n",
        "    return s"
      ],
      "metadata": {
        "id": "-9-5xXlDou2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gradio GUI ---\n",
        "iface = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"ここにプロンプトを入力してください\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"Llama-3.1-Swallow-8B-Instruct-v0.1 Fine-tuned Model\",\n",
        "    description=\"ファインチューニングされたモデルで文章を生成します。\"\n",
        ")\n",
        "\n",
        "# GUIの起動\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "WwJx5cumqB_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.**Colabランタイムの強制終了（オプション）**"
      ],
      "metadata": {
        "id": "vVb-6qtvkQTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Colabラインタイムの強制終了（オプション）\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "OuoxJoQDZ-oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "- [QLORA:Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "- [【Llama3】SFTTrainerで簡単ファインチューニング(QLoRA)](https://highreso.jp/edgehub/machinelearning/llama3sftqlora.html)\n",
        "- [huggingface/TRLのSFTTrainerクラスを使えばLLMのInstruction Tuningのコードがスッキリ書けてとても便利です](https://qiita.com/m__k/items/23ced0db6846e97d41cd)\n",
        "- [TRL - Transformer Reinforcement Learning](https://huggingface.co/docs/trl/index)\n",
        "- [Google Colabによる Llama3.2 / Qwen2.5 の ファインチューニング・ハンズオン](https://www.youtube.com/watch?v=fp4GC6OUZGc)"
      ],
      "metadata": {
        "id": "fXMFCSq4aOT1"
      }
    }
  ]
}